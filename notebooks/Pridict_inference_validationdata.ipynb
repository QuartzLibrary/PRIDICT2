{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = os.path.join(os.path.abspath('../'))\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,repo_dir)\n",
    "import pridict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth = os.path.join(repo_dir, 'dataset')\n",
    "data_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pth = os.path.join(repo_dir, 'input')\n",
    "input_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pridict.pridictv2.utilities import *\n",
    "from pridict.pridictv2.dataset import *\n",
    "from pridict.pridictv2.predict_outcomedistrib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-relationship",
   "metadata": {},
   "source": [
    "### Running on 29k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.read_csv(os.path.join(input_pth, '20240113_librarydiv_df_batchfile_with_adapted_wide_initial_target_with_HEKaverageedited.csv'))\n",
    "test_df['deepeditposition_lst'] = test_df['deepeditposition_lst'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['AdVaverageedited'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['HEKaverageedited'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-government",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(pred_df, dset_names, run_num, model_id, correction_type='all', wsize=20, outcome_names=['averageedited', 'averageunedited', 'averageindel']):\n",
    "    res_lst = []\n",
    "    mscore, report = compute_performance_multidata_from_df(pred_df, dset_names, outcome_names)\n",
    "    for i_data, dsetname in enumerate(dset_names):\n",
    "        m = mscore.modelscores_lst[i_data]\n",
    "        for tindx, tcol in enumerate(outcome_names):\n",
    "            pearson_score = m.pearson_lst[tindx]\n",
    "            spearman_score =  m.spearman_lst[tindx]\n",
    "            res_lst.append([model_id, wsize, run_num, pearson_score, spearman_score, tcol, dsetname, correction_type])\n",
    "    return res_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modelruns_avg_predictions(pred_df_lst, grouping_colnames=['seq_id', 'dataset_name']):\n",
    "    pred_df_allruns = pd.concat(pred_df_lst, axis=0, ignore_index=True)\n",
    "    agg_df = pred_df_allruns.groupby(by=grouping_colnames).mean()\n",
    "    agg_df.reset_index(inplace=True)\n",
    "    for colname in ('run_num', 'Unnamed: 0'):\n",
    "        if colname in agg_df:\n",
    "            del agg_df[colname]\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-cycling",
   "metadata": {},
   "source": [
    "### Evaluate all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device(True,0)\n",
    "\n",
    "include_MFE=False\n",
    "include_addendumfeat=False\n",
    "\n",
    "y_ref_colnames = ['HEKaverageedited']\n",
    "y_pred_colnames = ['averageedited', 'averageunedited']\n",
    "\n",
    "res_lst = []\n",
    "\n",
    "topfname = 'pridict_inference_29k_basemodels'\n",
    "\n",
    "\n",
    "# updated retrained new models\n",
    "dumpfname = 'newmodels_all'\n",
    "\n",
    "models = [('base_90k', 'pe_rnn_kldiv', 'exp_2023-06-02_09-49-21', ['HEK'], 5),\n",
    "          ('base_390k', 'pe_rnn_distribution_multidata', 'exp_2023-08-26_20-58-14',['HEKschwank', 'HEKhyongbum'], 5)\n",
    "         ]\n",
    "\n",
    "for model in models:\n",
    "    for wsize in [20]:\n",
    "        prieml_model = PRIEML_Model(device, \n",
    "                                    wsize=wsize, \n",
    "                                    normalize='max', \n",
    "                                    include_MFE=include_MFE, \n",
    "                                    include_addendumfeat=include_addendumfeat,\n",
    "                                    fdtype=torch.float32)\n",
    "\n",
    "        model_id, model_type, mfolder, cell_types, num_runs = model\n",
    "        dloader = prieml_model.prepare_data(test_df, \n",
    "                                            model_id,\n",
    "                                            cell_types=cell_types,\n",
    "                                            y_ref=y_ref_colnames, \n",
    "                                            batch_size=1500)\n",
    "        pred_df_lst = []\n",
    "        for run_num in range(num_runs):\n",
    "            model_dir = os.path.join(repo_dir, \n",
    "                                     'trained_models',\n",
    "                                     model_id,\n",
    "                                     mfolder,\n",
    "                                     'train_val')\n",
    "            mdir = os.path.join(model_dir, f'run_{run_num}')\n",
    "            print(mdir)\n",
    "            pred_df = prieml_model.predict_from_dloader(dloader, mdir, y_ref=y_pred_colnames)\n",
    "            pred_df['run_num'] = run_num\n",
    "            \n",
    "            # drop na \n",
    "            pred_df = pred_df.dropna(axis=0, subset= ['true_averageedited'])\n",
    "            pred_df_lst.append(pred_df)\n",
    "\n",
    "            mid = f\"{model_id}_{mfolder}\"\n",
    "            res = compute_performance(pred_df, \n",
    "                                      cell_types,\n",
    "                                      run_num,\n",
    "                                      mid,\n",
    "                                      wsize=wsize,\n",
    "                                      outcome_names=['averageedited'])\n",
    "            res_lst.extend(res)\n",
    "\n",
    "        # compute average prediction from multiple runs \n",
    "        agg_df = compute_modelruns_avg_predictions(pred_df_lst)\n",
    "        res = compute_performance(agg_df, \n",
    "                                  cell_types,\n",
    "                                 'avg_run',\n",
    "                                  mid,\n",
    "                                  wsize=wsize,\n",
    "                                  outcome_names=['averageedited'])\n",
    "        res_lst.extend(res)\n",
    "\n",
    "dump_dir = create_directory(os.path.join(repo_dir,\n",
    "                                         'experiments',\n",
    "                                         topfname,\n",
    "                                        dumpfname))\n",
    "res_df = pd.DataFrame(res_lst)\n",
    "res_df.columns = ['model_id', 'wsize', 'run_num', 'pear_score', 'spearman_score', 'outcome_name', 'cell_type', 'correction_type']\n",
    "res_df.to_csv(os.path.join(dump_dir, 'res_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.loc[res_df['run_num'] != 'avg_run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.loc[res_df['run_num'] == 'avg_run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = res_df['run_num'] != 'avg_run'\n",
    "res_df.loc[cond].groupby(by=['model_id', 'wsize', 'outcome_name', 'cell_type', 'correction_type'])[['pear_score', 'spearman_score']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-earth",
   "metadata": {},
   "source": [
    "### Visualizing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['seq_id'] = [f\"seq_{i}\" for i in range(test_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-raising",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose ids of sequences to visualize\n",
    "# we are using seq_id as the main column to filter from\n",
    "viz_res = prieml_model.visualize_seqs(test_df, ['seq_10','seq_50'])\n",
    "for kelm in viz_res:\n",
    "    display(HTML(viz_res[kelm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-qatar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
