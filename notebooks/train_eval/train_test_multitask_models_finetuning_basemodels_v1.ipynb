{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crazy-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "starting-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hazardous-friendly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/nicol/Documents/GitHub/wsl_experimental_pridict2/experimental_pridict2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir = os.path.join(os.path.abspath('../../'))\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "buried-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,repo_dir)\n",
    "import pridict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sound-pizza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/nicol/Documents/GitHub/wsl_experimental_pridict2/experimental_pridict2/dataset'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pth = os.path.join(repo_dir, 'dataset')\n",
    "data_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "valuable-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pridict.pridictv2.utilities import *\n",
    "from pridict.pridictv2.dataset import *\n",
    "from pridict.pridictv2.run_workflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "civil-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs available: 1\n",
      "cuda:0, name:NVIDIA GeForce GTX 1650\n",
      "total memory available: 3.999755859375 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nimath/miniconda3/envs/pridict2/lib/python3.10/site-packages/torch/cuda/memory.py:440: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n",
      "/home/nimath/miniconda3/envs/pridict2/lib/python3.10/site-packages/torch/cuda/memory.py:449: FutureWarning: torch.cuda.max_memory_cached has been renamed to torch.cuda.max_memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "report_available_cuda_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-diamond",
   "metadata": {},
   "source": [
    "### Generate datatensor partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pressing-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsize=20\n",
    "outcome_suffix = 'clamped'\n",
    "include_MFE = False\n",
    "if include_MFE:\n",
    "    fsuffix = 'withMFE'\n",
    "else:\n",
    "    fsuffix = 'withoutMFE'\n",
    "    \n",
    "# use these to tune on 23K library HEK, K562\n",
    "tfolder_name = 'proc_v2'\n",
    "dsetnames_lst = ['HEK', 'K562']\n",
    "\n",
    "data_dir = create_directory(os.path.join(repo_dir, 'dataset', tfolder_name, f'align_{fsuffix}'))\n",
    "\n",
    "dtensor_partitions_lst = []\n",
    "for outcome_name in dsetnames_lst:\n",
    "    fname = f'dpartitions_{outcome_name}_{outcome_suffix}_wsize{wsize}.pkl'\n",
    "    data_partitions =  ReaderWriter.read_data(os.path.join(data_dir, fname))\n",
    "    fname = f'dtensor_{outcome_name}_{outcome_suffix}_wsize{wsize}.pkl'\n",
    "    dtensor= ReaderWriter.read_data(os.path.join(data_dir, fname))\n",
    "    dtensor_partitions = generate_partition_datatensor(dtensor, data_partitions)\n",
    "    dtensor_partitions_lst.append(dtensor_partitions)\n",
    "    \n",
    "    \n",
    "dtensor_partitions_multidata = {}\n",
    "for run_num in range(5):\n",
    "    dtensor_partitions_multidata[run_num] = []\n",
    "    for dtensor_partitions in dtensor_partitions_lst:\n",
    "        dtensor_partitions_multidata[run_num].append(dtensor_partitions[run_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "divine-fiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164d750>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164d690>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164d5d0>},\n",
       "  {'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe137142800>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe0f165fdc0>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164fa90>}],\n",
       " 1: [{'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164d510>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164d450>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164d390>},\n",
       "  {'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164eb00>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164cf40>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164e1a0>}],\n",
       " 2: [{'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164d240>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164cdf0>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164c9d0>},\n",
       "  {'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164f910>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164f8b0>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164e590>}],\n",
       " 3: [{'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164e5f0>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164e7a0>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164ee60>},\n",
       "  {'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe13469d5a0>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe13469d360>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe13469c0a0>}],\n",
       " 4: [{'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164ed70>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164cc70>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe02164ef50>},\n",
       "  {'train': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe0018500d0>,\n",
       "   'validation': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe001850190>,\n",
       "   'test': <pridict.pridictv2.dataset.PartitionDataTensor at 0x7fe001850040>}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtensor_partitions_multidata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-dylan",
   "metadata": {},
   "source": [
    "### Define model and experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "white-petite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_gpu_map = {i:0 for i in range(len(data_partitions))}\n",
    "run_gpu_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-replication",
   "metadata": {},
   "source": [
    "### Specify which layers to finetune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impressed-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seqlevel_featembeder_HEK',\n",
       " 'decoder_HEK',\n",
       " 'global_featemb_init_attn_HEK',\n",
       " 'global_featemb_mut_attn_HEK',\n",
       " 'local_featemb_init_attn_HEK',\n",
       " 'local_featemb_mut_attn_HEK',\n",
       " 'seqlevel_featembeder_K562',\n",
       " 'decoder_K562',\n",
       " 'global_featemb_init_attn_K562',\n",
       " 'global_featemb_mut_attn_K562',\n",
       " 'local_featemb_init_attn_K562',\n",
       " 'local_featemb_mut_attn_K562']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_layernames = []\n",
    "\n",
    "for dsetname in dsetnames_lst:\n",
    "    for lname in ['seqlevel_featembeder',\n",
    "                  'decoder', \n",
    "                  'global_featemb_init_attn', \n",
    "                  'global_featemb_mut_attn', \n",
    "                  'local_featemb_init_attn',\n",
    "                  'local_featemb_mut_attn']:\n",
    "        trainable_layernames.append(f'{lname}_{dsetname}')\n",
    "        \n",
    "trainable_layernames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-brown",
   "metadata": {},
   "source": [
    "### Finetuning base models on 23K (HEK and K562) Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-origin",
   "metadata": {},
   "source": [
    "#### Use a base model pre-trained on Library 1 data (Mathis et al.) - see Figure 1 n in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ordered-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "z_dim = 72\n",
    "num_hidden_layers = 2\n",
    "bidirection=True\n",
    "p_dropout = 0.15\n",
    "rnn_class = nn.GRU\n",
    "nonlin_func = nn.ReLU()\n",
    "l2_reg = 1e-5\n",
    "batch_size = 750\n",
    "batch_size = 100\n",
    "num_epochs = 150\n",
    "# loss_func = 'KLDloss'\n",
    "loss_func = 'CEloss'\n",
    "# loss_func = 'Huberloss'\n",
    "trf_tup = [embed_dim, z_dim,\n",
    "           num_hidden_layers,\n",
    "           bidirection, \n",
    "           p_dropout,\n",
    "           rnn_class, nonlin_func,\n",
    "           l2_reg, batch_size, num_epochs]\n",
    "seqlevel_featdim = len(dtensor_partitions[0]['train'].pe_datatensor.seqlevel_feat_colnames)\n",
    "num_t_outcomes = 3\n",
    "default_outcomes = ['averageedited', 'averageunedited', 'averageindel']\n",
    "\n",
    "experiment_options = {'experiment_desc':'pe_rnn_distribution_multidata',\n",
    "                      'model_name':'PE_RNN_distribution_multidata',\n",
    "                      'annot_embed':8,\n",
    "                      'assemb_opt':'stack',\n",
    "                      'loader_mode':'cycle',\n",
    "                      'run_num':0,\n",
    "                      'fdtype':torch.float32,\n",
    "                      'wsize':wsize,\n",
    "                      'datasets_name':dsetnames_lst,\n",
    "                      'target_names': default_outcomes[:num_t_outcomes],\n",
    "                      'base_model_suffix':None,\n",
    "                      'separate_attention_layers':True,\n",
    "                      'separate_seqlevel_embedder':True,\n",
    "                      'seqlevel_featdim': seqlevel_featdim,\n",
    "                      'trainable_layernames': trainable_layernames,\n",
    "                      'num_outcomes':num_t_outcomes}\n",
    "mconfig, options = build_config_map(trf_tup, experiment_options, loss_func=loss_func)\n",
    "\n",
    "# provide the base model that will be used to fine-tune on the data\n",
    "# we will use base_90k (pretrained on Library 1) to finetune by specifying the folder name where the trained base model is found\n",
    "mfolder = 'exp_2023-06-02_09-49-21' # base_90k\n",
    "model_type = 'base_90k'\n",
    "trun = 1 # given that we have 5-fold training of base model we can specify which run to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-point",
   "metadata": {},
   "source": [
    "#### Use a base model pre-trained on Library 1 and Library-ClinVar data (Mathis et al. and Yu et al) - see Figure 1 n in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "younger-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######\n",
    "# ## uncomment this cell to use this configuration to finetune 390k base model\n",
    "# #######\n",
    "\n",
    "# ## 390k model retuning\n",
    "# embed_dim = 128\n",
    "# z_dim = 72\n",
    "# num_hidden_layers = 2\n",
    "# bidirection=True\n",
    "# p_dropout = 0.15\n",
    "# rnn_class = nn.GRU\n",
    "# nonlin_func = nn.ReLU()\n",
    "# l2_reg = 1e-5\n",
    "# batch_size = 750\n",
    "# num_epochs = 150\n",
    "# # loss_func = 'KLDloss'\n",
    "# loss_func = 'CEloss'\n",
    "# # loss_func = 'Huberloss'\n",
    "# trf_tup = [embed_dim, z_dim,\n",
    "#            num_hidden_layers,\n",
    "#            bidirection, \n",
    "#            p_dropout,\n",
    "#            rnn_class, nonlin_func,\n",
    "#            l2_reg, batch_size, num_epochs]\n",
    "# seqlevel_featdim = len(dtensor_partitions[0]['train'].pe_datatensor.seqlevel_feat_colnames)\n",
    "# default_outcomes = ['averageedited', 'averageunedited', 'averageindel']\n",
    "# num_t_outcomes = 3\n",
    "# experiment_options = {'experiment_desc':'pe_rnn_distribution_multidata',\n",
    "#                       'model_name':'PE_RNN_distribution_multidata',\n",
    "#                       'annot_embed':8,\n",
    "#                       'assemb_opt':'stack',\n",
    "#                       'loader_mode':'cycle',\n",
    "#                       'run_num':0,\n",
    "#                       'fdtype':torch.float32,\n",
    "#                       'wsize':wsize,\n",
    "#                       'datasets_name':dsetnames_lst,\n",
    "#                       'target_names': default_outcomes[:num_t_outcomes],\n",
    "#                       'base_model_suffix':'HEKschwank',\n",
    "#                       'separate_attention_layers':True,\n",
    "#                       'separate_seqlevel_embedder':True,\n",
    "#                       'seqlevel_featdim': seqlevel_featdim,\n",
    "#                       'trainable_layernames': trainable_layernames,\n",
    "#                       'num_outcomes':num_t_outcomes}\n",
    "# mconfig, options = build_config_map(trf_tup, experiment_options, loss_func=loss_func)\n",
    "# ## the base model 390k to finetune\n",
    "# mfolder = 'exp_2023-08-26_20-58-14' # folder name where pretrained model is found\n",
    "# model_type = 'base_390k'\n",
    "# trun = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "military-vinyl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/nicol/Documents/GitHub/wsl_experimental_pridict2/experimental_pridict2/experiments/pe_rnn_distribution_multidata'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "repo_path = create_directory(os.path.join(os.path.abspath('../')))\n",
    "experiment_desc = experiment_options['experiment_desc']\n",
    "exp_dir = create_directory(os.path.join(repo_dir, 'experiments', experiment_desc))\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-suffering",
   "metadata": {},
   "source": [
    "### Run training/fine-tuning on the 5-folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bigger-brief",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel_run: 1\n",
      "state_dict_dir: /mnt/c/Users/nicol/Documents/GitHub/wsl_experimental_pridict2/experimental_pridict2/trained_models/base_90k/exp_2023-06-02_09-49-21/train_val/run_1/model_statedict\n",
      "tr_val_dir: /mnt/c/Users/nicol/Documents/GitHub/wsl_experimental_pridict2/experimental_pridict2/experiments/pe_rnn_distribution_multidata/exp_2024-07-11_14-45-14\n",
      "\n",
      "config_exp_2024-07-11_14-45-14\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "m_name: init_annot_embed\n",
      "m_name_upd: init_annot_embed\n",
      "m_name: mut_annot_embed\n",
      "m_name_upd: mut_annot_embed\n",
      "m_name: init_encoder\n",
      "m_name_upd: init_encoder\n",
      "m_name: mut_encoder\n",
      "m_name_upd: mut_encoder\n",
      "m_name: seqlevel_featembeder_HEK\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: seqlevel_featembeder_K562\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: local_featemb_init_attn_HEK\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_HEK\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_HEK\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_HEK\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: local_featemb_init_attn_K562\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_K562\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_K562\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_K562\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: decoder_HEK\n",
      "m_name_upd: decoder\n",
      "m_name: decoder_K562\n",
      "m_name_upd: decoder\n",
      "init_annot_embed We.weight 768\n",
      "init_annot_embed Wproto.weight 32\n",
      "init_annot_embed Wpbs.weight 24\n",
      "init_annot_embed Wrt.weight 32\n",
      "mut_annot_embed We.weight 768\n",
      "mut_annot_embed Wpbs.weight 24\n",
      "mut_annot_embed Wrt.weight 32\n",
      "init_encoder rnn.weight_ih_l0 58368\n",
      "init_encoder rnn.weight_hh_l0 49152\n",
      "init_encoder rnn.bias_ih_l0 384\n",
      "init_encoder rnn.bias_hh_l0 384\n",
      "init_encoder rnn.weight_ih_l0_reverse 58368\n",
      "init_encoder rnn.weight_hh_l0_reverse 49152\n",
      "init_encoder rnn.bias_ih_l0_reverse 384\n",
      "init_encoder rnn.bias_hh_l0_reverse 384\n",
      "init_encoder rnn.weight_ih_l1 98304\n",
      "init_encoder rnn.weight_hh_l1 49152\n",
      "init_encoder rnn.bias_ih_l1 384\n",
      "init_encoder rnn.bias_hh_l1 384\n",
      "init_encoder rnn.weight_ih_l1_reverse 98304\n",
      "init_encoder rnn.weight_hh_l1_reverse 49152\n",
      "init_encoder rnn.bias_ih_l1_reverse 384\n",
      "init_encoder rnn.bias_hh_l1_reverse 384\n",
      "init_encoder Wz.weight 18432\n",
      "init_encoder Wz.bias 72\n",
      "mut_encoder rnn.weight_ih_l0 55296\n",
      "mut_encoder rnn.weight_hh_l0 49152\n",
      "mut_encoder rnn.bias_ih_l0 384\n",
      "mut_encoder rnn.bias_hh_l0 384\n",
      "mut_encoder rnn.weight_ih_l0_reverse 55296\n",
      "mut_encoder rnn.weight_hh_l0_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l0_reverse 384\n",
      "mut_encoder rnn.bias_hh_l0_reverse 384\n",
      "mut_encoder rnn.weight_ih_l1 98304\n",
      "mut_encoder rnn.weight_hh_l1 49152\n",
      "mut_encoder rnn.bias_ih_l1 384\n",
      "mut_encoder rnn.bias_hh_l1 384\n",
      "mut_encoder rnn.weight_ih_l1_reverse 98304\n",
      "mut_encoder rnn.weight_hh_l1_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l1_reverse 384\n",
      "mut_encoder rnn.bias_hh_l1_reverse 384\n",
      "mut_encoder Wz.weight 18432\n",
      "mut_encoder Wz.bias 72\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model: 1177830.0\n",
      "init_annot_embed We.weight False\n",
      "init_annot_embed Wproto.weight False\n",
      "init_annot_embed Wpbs.weight False\n",
      "init_annot_embed Wrt.weight False\n",
      "\n",
      "mut_annot_embed We.weight False\n",
      "mut_annot_embed Wpbs.weight False\n",
      "mut_annot_embed Wrt.weight False\n",
      "\n",
      "init_encoder rnn.weight_ih_l0 False\n",
      "init_encoder rnn.weight_hh_l0 False\n",
      "init_encoder rnn.bias_ih_l0 False\n",
      "init_encoder rnn.bias_hh_l0 False\n",
      "init_encoder rnn.weight_ih_l0_reverse False\n",
      "init_encoder rnn.weight_hh_l0_reverse False\n",
      "init_encoder rnn.bias_ih_l0_reverse False\n",
      "init_encoder rnn.bias_hh_l0_reverse False\n",
      "init_encoder rnn.weight_ih_l1 False\n",
      "init_encoder rnn.weight_hh_l1 False\n",
      "init_encoder rnn.bias_ih_l1 False\n",
      "init_encoder rnn.bias_hh_l1 False\n",
      "init_encoder rnn.weight_ih_l1_reverse False\n",
      "init_encoder rnn.weight_hh_l1_reverse False\n",
      "init_encoder rnn.bias_ih_l1_reverse False\n",
      "init_encoder rnn.bias_hh_l1_reverse False\n",
      "init_encoder Wz.weight False\n",
      "init_encoder Wz.bias False\n",
      "\n",
      "mut_encoder rnn.weight_ih_l0 False\n",
      "mut_encoder rnn.weight_hh_l0 False\n",
      "mut_encoder rnn.bias_ih_l0 False\n",
      "mut_encoder rnn.bias_hh_l0 False\n",
      "mut_encoder rnn.weight_ih_l0_reverse False\n",
      "mut_encoder rnn.weight_hh_l0_reverse False\n",
      "mut_encoder rnn.bias_ih_l0_reverse False\n",
      "mut_encoder rnn.bias_hh_l0_reverse False\n",
      "mut_encoder rnn.weight_ih_l1 False\n",
      "mut_encoder rnn.weight_hh_l1 False\n",
      "mut_encoder rnn.bias_ih_l1 False\n",
      "mut_encoder rnn.bias_hh_l1 False\n",
      "mut_encoder rnn.weight_ih_l1_reverse False\n",
      "mut_encoder rnn.weight_hh_l1_reverse False\n",
      "mut_encoder rnn.bias_ih_l1_reverse False\n",
      "mut_encoder rnn.bias_hh_l1_reverse False\n",
      "mut_encoder Wz.weight False\n",
      "mut_encoder Wz.bias False\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight True\n",
      "seqlevel_featembeder_HEK We.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "seqlevel_featembeder_K562 We.weight True\n",
      "seqlevel_featembeder_K562 We.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "local_featemb_init_attn_HEK queryv True\n",
      "\n",
      "global_featemb_init_attn_HEK queryv True\n",
      "\n",
      "local_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "global_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "local_featemb_init_attn_K562 queryv True\n",
      "\n",
      "global_featemb_init_attn_K562 queryv True\n",
      "\n",
      "local_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "global_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "decoder_HEK We.weight True\n",
      "decoder_HEK We.bias True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_HEK W_mu.weight True\n",
      "decoder_HEK W_mu.bias True\n",
      "\n",
      "decoder_K562 We.weight True\n",
      "decoder_K562 We.bias True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_K562 W_mu.weight True\n",
      "decoder_K562 W_mu.bias True\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model after freezing: 119238.0\n",
      "119238\n",
      "119238\n",
      "\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 0 | epoch: 0 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:33<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32500\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 0 | epoch: 0 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:04<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3674\n",
      "updated # of rows: 3674\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 0 | epoch: 1 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [02:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32500\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 0 | epoch: 1 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:03<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3674\n",
      "updated # of rows: 3674\n",
      "config_exp_2024-07-11_14-45-14\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "m_name: init_annot_embed\n",
      "m_name_upd: init_annot_embed\n",
      "m_name: mut_annot_embed\n",
      "m_name_upd: mut_annot_embed\n",
      "m_name: init_encoder\n",
      "m_name_upd: init_encoder\n",
      "m_name: mut_encoder\n",
      "m_name_upd: mut_encoder\n",
      "m_name: seqlevel_featembeder_HEK\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: seqlevel_featembeder_K562\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: local_featemb_init_attn_HEK\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_HEK\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_HEK\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_HEK\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: local_featemb_init_attn_K562\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_K562\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_K562\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_K562\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: decoder_HEK\n",
      "m_name_upd: decoder\n",
      "m_name: decoder_K562\n",
      "m_name_upd: decoder\n",
      "init_annot_embed We.weight 768\n",
      "init_annot_embed Wproto.weight 32\n",
      "init_annot_embed Wpbs.weight 24\n",
      "init_annot_embed Wrt.weight 32\n",
      "mut_annot_embed We.weight 768\n",
      "mut_annot_embed Wpbs.weight 24\n",
      "mut_annot_embed Wrt.weight 32\n",
      "init_encoder rnn.weight_ih_l0 58368\n",
      "init_encoder rnn.weight_hh_l0 49152\n",
      "init_encoder rnn.bias_ih_l0 384\n",
      "init_encoder rnn.bias_hh_l0 384\n",
      "init_encoder rnn.weight_ih_l0_reverse 58368\n",
      "init_encoder rnn.weight_hh_l0_reverse 49152\n",
      "init_encoder rnn.bias_ih_l0_reverse 384\n",
      "init_encoder rnn.bias_hh_l0_reverse 384\n",
      "init_encoder rnn.weight_ih_l1 98304\n",
      "init_encoder rnn.weight_hh_l1 49152\n",
      "init_encoder rnn.bias_ih_l1 384\n",
      "init_encoder rnn.bias_hh_l1 384\n",
      "init_encoder rnn.weight_ih_l1_reverse 98304\n",
      "init_encoder rnn.weight_hh_l1_reverse 49152\n",
      "init_encoder rnn.bias_ih_l1_reverse 384\n",
      "init_encoder rnn.bias_hh_l1_reverse 384\n",
      "init_encoder Wz.weight 18432\n",
      "init_encoder Wz.bias 72\n",
      "mut_encoder rnn.weight_ih_l0 55296\n",
      "mut_encoder rnn.weight_hh_l0 49152\n",
      "mut_encoder rnn.bias_ih_l0 384\n",
      "mut_encoder rnn.bias_hh_l0 384\n",
      "mut_encoder rnn.weight_ih_l0_reverse 55296\n",
      "mut_encoder rnn.weight_hh_l0_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l0_reverse 384\n",
      "mut_encoder rnn.bias_hh_l0_reverse 384\n",
      "mut_encoder rnn.weight_ih_l1 98304\n",
      "mut_encoder rnn.weight_hh_l1 49152\n",
      "mut_encoder rnn.bias_ih_l1 384\n",
      "mut_encoder rnn.bias_hh_l1 384\n",
      "mut_encoder rnn.weight_ih_l1_reverse 98304\n",
      "mut_encoder rnn.weight_hh_l1_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l1_reverse 384\n",
      "mut_encoder rnn.bias_hh_l1_reverse 384\n",
      "mut_encoder Wz.weight 18432\n",
      "mut_encoder Wz.bias 72\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model: 1177830.0\n",
      "init_annot_embed We.weight False\n",
      "init_annot_embed Wproto.weight False\n",
      "init_annot_embed Wpbs.weight False\n",
      "init_annot_embed Wrt.weight False\n",
      "\n",
      "mut_annot_embed We.weight False\n",
      "mut_annot_embed Wpbs.weight False\n",
      "mut_annot_embed Wrt.weight False\n",
      "\n",
      "init_encoder rnn.weight_ih_l0 False\n",
      "init_encoder rnn.weight_hh_l0 False\n",
      "init_encoder rnn.bias_ih_l0 False\n",
      "init_encoder rnn.bias_hh_l0 False\n",
      "init_encoder rnn.weight_ih_l0_reverse False\n",
      "init_encoder rnn.weight_hh_l0_reverse False\n",
      "init_encoder rnn.bias_ih_l0_reverse False\n",
      "init_encoder rnn.bias_hh_l0_reverse False\n",
      "init_encoder rnn.weight_ih_l1 False\n",
      "init_encoder rnn.weight_hh_l1 False\n",
      "init_encoder rnn.bias_ih_l1 False\n",
      "init_encoder rnn.bias_hh_l1 False\n",
      "init_encoder rnn.weight_ih_l1_reverse False\n",
      "init_encoder rnn.weight_hh_l1_reverse False\n",
      "init_encoder rnn.bias_ih_l1_reverse False\n",
      "init_encoder rnn.bias_hh_l1_reverse False\n",
      "init_encoder Wz.weight False\n",
      "init_encoder Wz.bias False\n",
      "\n",
      "mut_encoder rnn.weight_ih_l0 False\n",
      "mut_encoder rnn.weight_hh_l0 False\n",
      "mut_encoder rnn.bias_ih_l0 False\n",
      "mut_encoder rnn.bias_hh_l0 False\n",
      "mut_encoder rnn.weight_ih_l0_reverse False\n",
      "mut_encoder rnn.weight_hh_l0_reverse False\n",
      "mut_encoder rnn.bias_ih_l0_reverse False\n",
      "mut_encoder rnn.bias_hh_l0_reverse False\n",
      "mut_encoder rnn.weight_ih_l1 False\n",
      "mut_encoder rnn.weight_hh_l1 False\n",
      "mut_encoder rnn.bias_ih_l1 False\n",
      "mut_encoder rnn.bias_hh_l1 False\n",
      "mut_encoder rnn.weight_ih_l1_reverse False\n",
      "mut_encoder rnn.weight_hh_l1_reverse False\n",
      "mut_encoder rnn.bias_ih_l1_reverse False\n",
      "mut_encoder rnn.bias_hh_l1_reverse False\n",
      "mut_encoder Wz.weight False\n",
      "mut_encoder Wz.bias False\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight True\n",
      "seqlevel_featembeder_HEK We.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "seqlevel_featembeder_K562 We.weight True\n",
      "seqlevel_featembeder_K562 We.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "local_featemb_init_attn_HEK queryv True\n",
      "\n",
      "global_featemb_init_attn_HEK queryv True\n",
      "\n",
      "local_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "global_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "local_featemb_init_attn_K562 queryv True\n",
      "\n",
      "global_featemb_init_attn_K562 queryv True\n",
      "\n",
      "local_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "global_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "decoder_HEK We.weight True\n",
      "decoder_HEK We.bias True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_HEK W_mu.weight True\n",
      "decoder_HEK W_mu.bias True\n",
      "\n",
      "decoder_K562 We.weight True\n",
      "decoder_K562 We.bias True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_K562 W_mu.weight True\n",
      "decoder_K562 W_mu.bias True\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model after freezing: 119238.0\n",
      "119238\n",
      "119238\n",
      "\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 1 | epoch: 0 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:41<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32600\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 1 | epoch: 0 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:04<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3699\n",
      "updated # of rows: 3599\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 1 | epoch: 1 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:43<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32600\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 1 | epoch: 1 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:06<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3699\n",
      "updated # of rows: 3599\n",
      "config_exp_2024-07-11_14-45-14\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "m_name: init_annot_embed\n",
      "m_name_upd: init_annot_embed\n",
      "m_name: mut_annot_embed\n",
      "m_name_upd: mut_annot_embed\n",
      "m_name: init_encoder\n",
      "m_name_upd: init_encoder\n",
      "m_name: mut_encoder\n",
      "m_name_upd: mut_encoder\n",
      "m_name: seqlevel_featembeder_HEK\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: seqlevel_featembeder_K562\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: local_featemb_init_attn_HEK\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_HEK\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_HEK\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_HEK\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: local_featemb_init_attn_K562\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_K562\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_K562\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_K562\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: decoder_HEK\n",
      "m_name_upd: decoder\n",
      "m_name: decoder_K562\n",
      "m_name_upd: decoder\n",
      "init_annot_embed We.weight 768\n",
      "init_annot_embed Wproto.weight 32\n",
      "init_annot_embed Wpbs.weight 24\n",
      "init_annot_embed Wrt.weight 32\n",
      "mut_annot_embed We.weight 768\n",
      "mut_annot_embed Wpbs.weight 24\n",
      "mut_annot_embed Wrt.weight 32\n",
      "init_encoder rnn.weight_ih_l0 58368\n",
      "init_encoder rnn.weight_hh_l0 49152\n",
      "init_encoder rnn.bias_ih_l0 384\n",
      "init_encoder rnn.bias_hh_l0 384\n",
      "init_encoder rnn.weight_ih_l0_reverse 58368\n",
      "init_encoder rnn.weight_hh_l0_reverse 49152\n",
      "init_encoder rnn.bias_ih_l0_reverse 384\n",
      "init_encoder rnn.bias_hh_l0_reverse 384\n",
      "init_encoder rnn.weight_ih_l1 98304\n",
      "init_encoder rnn.weight_hh_l1 49152\n",
      "init_encoder rnn.bias_ih_l1 384\n",
      "init_encoder rnn.bias_hh_l1 384\n",
      "init_encoder rnn.weight_ih_l1_reverse 98304\n",
      "init_encoder rnn.weight_hh_l1_reverse 49152\n",
      "init_encoder rnn.bias_ih_l1_reverse 384\n",
      "init_encoder rnn.bias_hh_l1_reverse 384\n",
      "init_encoder Wz.weight 18432\n",
      "init_encoder Wz.bias 72\n",
      "mut_encoder rnn.weight_ih_l0 55296\n",
      "mut_encoder rnn.weight_hh_l0 49152\n",
      "mut_encoder rnn.bias_ih_l0 384\n",
      "mut_encoder rnn.bias_hh_l0 384\n",
      "mut_encoder rnn.weight_ih_l0_reverse 55296\n",
      "mut_encoder rnn.weight_hh_l0_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l0_reverse 384\n",
      "mut_encoder rnn.bias_hh_l0_reverse 384\n",
      "mut_encoder rnn.weight_ih_l1 98304\n",
      "mut_encoder rnn.weight_hh_l1 49152\n",
      "mut_encoder rnn.bias_ih_l1 384\n",
      "mut_encoder rnn.bias_hh_l1 384\n",
      "mut_encoder rnn.weight_ih_l1_reverse 98304\n",
      "mut_encoder rnn.weight_hh_l1_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l1_reverse 384\n",
      "mut_encoder rnn.bias_hh_l1_reverse 384\n",
      "mut_encoder Wz.weight 18432\n",
      "mut_encoder Wz.bias 72\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model: 1177830.0\n",
      "init_annot_embed We.weight False\n",
      "init_annot_embed Wproto.weight False\n",
      "init_annot_embed Wpbs.weight False\n",
      "init_annot_embed Wrt.weight False\n",
      "\n",
      "mut_annot_embed We.weight False\n",
      "mut_annot_embed Wpbs.weight False\n",
      "mut_annot_embed Wrt.weight False\n",
      "\n",
      "init_encoder rnn.weight_ih_l0 False\n",
      "init_encoder rnn.weight_hh_l0 False\n",
      "init_encoder rnn.bias_ih_l0 False\n",
      "init_encoder rnn.bias_hh_l0 False\n",
      "init_encoder rnn.weight_ih_l0_reverse False\n",
      "init_encoder rnn.weight_hh_l0_reverse False\n",
      "init_encoder rnn.bias_ih_l0_reverse False\n",
      "init_encoder rnn.bias_hh_l0_reverse False\n",
      "init_encoder rnn.weight_ih_l1 False\n",
      "init_encoder rnn.weight_hh_l1 False\n",
      "init_encoder rnn.bias_ih_l1 False\n",
      "init_encoder rnn.bias_hh_l1 False\n",
      "init_encoder rnn.weight_ih_l1_reverse False\n",
      "init_encoder rnn.weight_hh_l1_reverse False\n",
      "init_encoder rnn.bias_ih_l1_reverse False\n",
      "init_encoder rnn.bias_hh_l1_reverse False\n",
      "init_encoder Wz.weight False\n",
      "init_encoder Wz.bias False\n",
      "\n",
      "mut_encoder rnn.weight_ih_l0 False\n",
      "mut_encoder rnn.weight_hh_l0 False\n",
      "mut_encoder rnn.bias_ih_l0 False\n",
      "mut_encoder rnn.bias_hh_l0 False\n",
      "mut_encoder rnn.weight_ih_l0_reverse False\n",
      "mut_encoder rnn.weight_hh_l0_reverse False\n",
      "mut_encoder rnn.bias_ih_l0_reverse False\n",
      "mut_encoder rnn.bias_hh_l0_reverse False\n",
      "mut_encoder rnn.weight_ih_l1 False\n",
      "mut_encoder rnn.weight_hh_l1 False\n",
      "mut_encoder rnn.bias_ih_l1 False\n",
      "mut_encoder rnn.bias_hh_l1 False\n",
      "mut_encoder rnn.weight_ih_l1_reverse False\n",
      "mut_encoder rnn.weight_hh_l1_reverse False\n",
      "mut_encoder rnn.bias_ih_l1_reverse False\n",
      "mut_encoder rnn.bias_hh_l1_reverse False\n",
      "mut_encoder Wz.weight False\n",
      "mut_encoder Wz.bias False\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight True\n",
      "seqlevel_featembeder_HEK We.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "seqlevel_featembeder_K562 We.weight True\n",
      "seqlevel_featembeder_K562 We.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "local_featemb_init_attn_HEK queryv True\n",
      "\n",
      "global_featemb_init_attn_HEK queryv True\n",
      "\n",
      "local_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "global_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "local_featemb_init_attn_K562 queryv True\n",
      "\n",
      "global_featemb_init_attn_K562 queryv True\n",
      "\n",
      "local_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "global_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "decoder_HEK We.weight True\n",
      "decoder_HEK We.bias True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_HEK W_mu.weight True\n",
      "decoder_HEK W_mu.bias True\n",
      "\n",
      "decoder_K562 We.weight True\n",
      "decoder_K562 We.bias True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_K562 W_mu.weight True\n",
      "decoder_K562 W_mu.bias True\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model after freezing: 119238.0\n",
      "119238\n",
      "119238\n",
      "\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 2 | epoch: 0 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:37<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32500\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 2 | epoch: 0 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:03<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3642\n",
      "updated # of rows: 3642\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 2 | epoch: 1 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:47<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32500\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 2 | epoch: 1 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:06<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3642\n",
      "updated # of rows: 3642\n",
      "config_exp_2024-07-11_14-45-14\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "m_name: init_annot_embed\n",
      "m_name_upd: init_annot_embed\n",
      "m_name: mut_annot_embed\n",
      "m_name_upd: mut_annot_embed\n",
      "m_name: init_encoder\n",
      "m_name_upd: init_encoder\n",
      "m_name: mut_encoder\n",
      "m_name_upd: mut_encoder\n",
      "m_name: seqlevel_featembeder_HEK\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: seqlevel_featembeder_K562\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: local_featemb_init_attn_HEK\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_HEK\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_HEK\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_HEK\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: local_featemb_init_attn_K562\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_K562\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_K562\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_K562\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: decoder_HEK\n",
      "m_name_upd: decoder\n",
      "m_name: decoder_K562\n",
      "m_name_upd: decoder\n",
      "init_annot_embed We.weight 768\n",
      "init_annot_embed Wproto.weight 32\n",
      "init_annot_embed Wpbs.weight 24\n",
      "init_annot_embed Wrt.weight 32\n",
      "mut_annot_embed We.weight 768\n",
      "mut_annot_embed Wpbs.weight 24\n",
      "mut_annot_embed Wrt.weight 32\n",
      "init_encoder rnn.weight_ih_l0 58368\n",
      "init_encoder rnn.weight_hh_l0 49152\n",
      "init_encoder rnn.bias_ih_l0 384\n",
      "init_encoder rnn.bias_hh_l0 384\n",
      "init_encoder rnn.weight_ih_l0_reverse 58368\n",
      "init_encoder rnn.weight_hh_l0_reverse 49152\n",
      "init_encoder rnn.bias_ih_l0_reverse 384\n",
      "init_encoder rnn.bias_hh_l0_reverse 384\n",
      "init_encoder rnn.weight_ih_l1 98304\n",
      "init_encoder rnn.weight_hh_l1 49152\n",
      "init_encoder rnn.bias_ih_l1 384\n",
      "init_encoder rnn.bias_hh_l1 384\n",
      "init_encoder rnn.weight_ih_l1_reverse 98304\n",
      "init_encoder rnn.weight_hh_l1_reverse 49152\n",
      "init_encoder rnn.bias_ih_l1_reverse 384\n",
      "init_encoder rnn.bias_hh_l1_reverse 384\n",
      "init_encoder Wz.weight 18432\n",
      "init_encoder Wz.bias 72\n",
      "mut_encoder rnn.weight_ih_l0 55296\n",
      "mut_encoder rnn.weight_hh_l0 49152\n",
      "mut_encoder rnn.bias_ih_l0 384\n",
      "mut_encoder rnn.bias_hh_l0 384\n",
      "mut_encoder rnn.weight_ih_l0_reverse 55296\n",
      "mut_encoder rnn.weight_hh_l0_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l0_reverse 384\n",
      "mut_encoder rnn.bias_hh_l0_reverse 384\n",
      "mut_encoder rnn.weight_ih_l1 98304\n",
      "mut_encoder rnn.weight_hh_l1 49152\n",
      "mut_encoder rnn.bias_ih_l1 384\n",
      "mut_encoder rnn.bias_hh_l1 384\n",
      "mut_encoder rnn.weight_ih_l1_reverse 98304\n",
      "mut_encoder rnn.weight_hh_l1_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l1_reverse 384\n",
      "mut_encoder rnn.bias_hh_l1_reverse 384\n",
      "mut_encoder Wz.weight 18432\n",
      "mut_encoder Wz.bias 72\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model: 1177830.0\n",
      "init_annot_embed We.weight False\n",
      "init_annot_embed Wproto.weight False\n",
      "init_annot_embed Wpbs.weight False\n",
      "init_annot_embed Wrt.weight False\n",
      "\n",
      "mut_annot_embed We.weight False\n",
      "mut_annot_embed Wpbs.weight False\n",
      "mut_annot_embed Wrt.weight False\n",
      "\n",
      "init_encoder rnn.weight_ih_l0 False\n",
      "init_encoder rnn.weight_hh_l0 False\n",
      "init_encoder rnn.bias_ih_l0 False\n",
      "init_encoder rnn.bias_hh_l0 False\n",
      "init_encoder rnn.weight_ih_l0_reverse False\n",
      "init_encoder rnn.weight_hh_l0_reverse False\n",
      "init_encoder rnn.bias_ih_l0_reverse False\n",
      "init_encoder rnn.bias_hh_l0_reverse False\n",
      "init_encoder rnn.weight_ih_l1 False\n",
      "init_encoder rnn.weight_hh_l1 False\n",
      "init_encoder rnn.bias_ih_l1 False\n",
      "init_encoder rnn.bias_hh_l1 False\n",
      "init_encoder rnn.weight_ih_l1_reverse False\n",
      "init_encoder rnn.weight_hh_l1_reverse False\n",
      "init_encoder rnn.bias_ih_l1_reverse False\n",
      "init_encoder rnn.bias_hh_l1_reverse False\n",
      "init_encoder Wz.weight False\n",
      "init_encoder Wz.bias False\n",
      "\n",
      "mut_encoder rnn.weight_ih_l0 False\n",
      "mut_encoder rnn.weight_hh_l0 False\n",
      "mut_encoder rnn.bias_ih_l0 False\n",
      "mut_encoder rnn.bias_hh_l0 False\n",
      "mut_encoder rnn.weight_ih_l0_reverse False\n",
      "mut_encoder rnn.weight_hh_l0_reverse False\n",
      "mut_encoder rnn.bias_ih_l0_reverse False\n",
      "mut_encoder rnn.bias_hh_l0_reverse False\n",
      "mut_encoder rnn.weight_ih_l1 False\n",
      "mut_encoder rnn.weight_hh_l1 False\n",
      "mut_encoder rnn.bias_ih_l1 False\n",
      "mut_encoder rnn.bias_hh_l1 False\n",
      "mut_encoder rnn.weight_ih_l1_reverse False\n",
      "mut_encoder rnn.weight_hh_l1_reverse False\n",
      "mut_encoder rnn.bias_ih_l1_reverse False\n",
      "mut_encoder rnn.bias_hh_l1_reverse False\n",
      "mut_encoder Wz.weight False\n",
      "mut_encoder Wz.bias False\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight True\n",
      "seqlevel_featembeder_HEK We.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "seqlevel_featembeder_K562 We.weight True\n",
      "seqlevel_featembeder_K562 We.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "local_featemb_init_attn_HEK queryv True\n",
      "\n",
      "global_featemb_init_attn_HEK queryv True\n",
      "\n",
      "local_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "global_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "local_featemb_init_attn_K562 queryv True\n",
      "\n",
      "global_featemb_init_attn_K562 queryv True\n",
      "\n",
      "local_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "global_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "decoder_HEK We.weight True\n",
      "decoder_HEK We.bias True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_HEK W_mu.weight True\n",
      "decoder_HEK W_mu.bias True\n",
      "\n",
      "decoder_K562 We.weight True\n",
      "decoder_K562 We.bias True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_K562 W_mu.weight True\n",
      "decoder_K562 W_mu.bias True\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model after freezing: 119238.0\n",
      "119238\n",
      "119238\n",
      "\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 3 | epoch: 0 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:54<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32500\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 3 | epoch: 0 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:07<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3665\n",
      "updated # of rows: 3665\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 3 | epoch: 1 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:36<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32500\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 3 | epoch: 1 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:04<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3665\n",
      "updated # of rows: 3665\n",
      "config_exp_2024-07-11_14-45-14\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "m_name: init_annot_embed\n",
      "m_name_upd: init_annot_embed\n",
      "m_name: mut_annot_embed\n",
      "m_name_upd: mut_annot_embed\n",
      "m_name: init_encoder\n",
      "m_name_upd: init_encoder\n",
      "m_name: mut_encoder\n",
      "m_name_upd: mut_encoder\n",
      "m_name: seqlevel_featembeder_HEK\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: seqlevel_featembeder_K562\n",
      "m_name_upd: seqlevel_featembeder\n",
      "m_name: local_featemb_init_attn_HEK\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_HEK\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_HEK\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_HEK\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: local_featemb_init_attn_K562\n",
      "m_name_upd: local_featemb_init_attn\n",
      "m_name: global_featemb_init_attn_K562\n",
      "m_name_upd: global_featemb_init_attn\n",
      "m_name: local_featemb_mut_attn_K562\n",
      "m_name_upd: local_featemb_mut_attn\n",
      "m_name: global_featemb_mut_attn_K562\n",
      "m_name_upd: global_featemb_mut_attn\n",
      "m_name: decoder_HEK\n",
      "m_name_upd: decoder\n",
      "m_name: decoder_K562\n",
      "m_name_upd: decoder\n",
      "init_annot_embed We.weight 768\n",
      "init_annot_embed Wproto.weight 32\n",
      "init_annot_embed Wpbs.weight 24\n",
      "init_annot_embed Wrt.weight 32\n",
      "mut_annot_embed We.weight 768\n",
      "mut_annot_embed Wpbs.weight 24\n",
      "mut_annot_embed Wrt.weight 32\n",
      "init_encoder rnn.weight_ih_l0 58368\n",
      "init_encoder rnn.weight_hh_l0 49152\n",
      "init_encoder rnn.bias_ih_l0 384\n",
      "init_encoder rnn.bias_hh_l0 384\n",
      "init_encoder rnn.weight_ih_l0_reverse 58368\n",
      "init_encoder rnn.weight_hh_l0_reverse 49152\n",
      "init_encoder rnn.bias_ih_l0_reverse 384\n",
      "init_encoder rnn.bias_hh_l0_reverse 384\n",
      "init_encoder rnn.weight_ih_l1 98304\n",
      "init_encoder rnn.weight_hh_l1 49152\n",
      "init_encoder rnn.bias_ih_l1 384\n",
      "init_encoder rnn.bias_hh_l1 384\n",
      "init_encoder rnn.weight_ih_l1_reverse 98304\n",
      "init_encoder rnn.weight_hh_l1_reverse 49152\n",
      "init_encoder rnn.bias_ih_l1_reverse 384\n",
      "init_encoder rnn.bias_hh_l1_reverse 384\n",
      "init_encoder Wz.weight 18432\n",
      "init_encoder Wz.bias 72\n",
      "mut_encoder rnn.weight_ih_l0 55296\n",
      "mut_encoder rnn.weight_hh_l0 49152\n",
      "mut_encoder rnn.bias_ih_l0 384\n",
      "mut_encoder rnn.bias_hh_l0 384\n",
      "mut_encoder rnn.weight_ih_l0_reverse 55296\n",
      "mut_encoder rnn.weight_hh_l0_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l0_reverse 384\n",
      "mut_encoder rnn.bias_hh_l0_reverse 384\n",
      "mut_encoder rnn.weight_ih_l1 98304\n",
      "mut_encoder rnn.weight_hh_l1 49152\n",
      "mut_encoder rnn.bias_ih_l1 384\n",
      "mut_encoder rnn.bias_hh_l1 384\n",
      "mut_encoder rnn.weight_ih_l1_reverse 98304\n",
      "mut_encoder rnn.weight_hh_l1_reverse 49152\n",
      "mut_encoder rnn.bias_ih_l1_reverse 384\n",
      "mut_encoder rnn.bias_hh_l1_reverse 384\n",
      "mut_encoder Wz.weight 18432\n",
      "mut_encoder Wz.bias 72\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model: 1177830.0\n",
      "init_annot_embed We.weight False\n",
      "init_annot_embed Wproto.weight False\n",
      "init_annot_embed Wpbs.weight False\n",
      "init_annot_embed Wrt.weight False\n",
      "\n",
      "mut_annot_embed We.weight False\n",
      "mut_annot_embed Wpbs.weight False\n",
      "mut_annot_embed Wrt.weight False\n",
      "\n",
      "init_encoder rnn.weight_ih_l0 False\n",
      "init_encoder rnn.weight_hh_l0 False\n",
      "init_encoder rnn.bias_ih_l0 False\n",
      "init_encoder rnn.bias_hh_l0 False\n",
      "init_encoder rnn.weight_ih_l0_reverse False\n",
      "init_encoder rnn.weight_hh_l0_reverse False\n",
      "init_encoder rnn.bias_ih_l0_reverse False\n",
      "init_encoder rnn.bias_hh_l0_reverse False\n",
      "init_encoder rnn.weight_ih_l1 False\n",
      "init_encoder rnn.weight_hh_l1 False\n",
      "init_encoder rnn.bias_ih_l1 False\n",
      "init_encoder rnn.bias_hh_l1 False\n",
      "init_encoder rnn.weight_ih_l1_reverse False\n",
      "init_encoder rnn.weight_hh_l1_reverse False\n",
      "init_encoder rnn.bias_ih_l1_reverse False\n",
      "init_encoder rnn.bias_hh_l1_reverse False\n",
      "init_encoder Wz.weight False\n",
      "init_encoder Wz.bias False\n",
      "\n",
      "mut_encoder rnn.weight_ih_l0 False\n",
      "mut_encoder rnn.weight_hh_l0 False\n",
      "mut_encoder rnn.bias_ih_l0 False\n",
      "mut_encoder rnn.bias_hh_l0 False\n",
      "mut_encoder rnn.weight_ih_l0_reverse False\n",
      "mut_encoder rnn.weight_hh_l0_reverse False\n",
      "mut_encoder rnn.bias_ih_l0_reverse False\n",
      "mut_encoder rnn.bias_hh_l0_reverse False\n",
      "mut_encoder rnn.weight_ih_l1 False\n",
      "mut_encoder rnn.weight_hh_l1 False\n",
      "mut_encoder rnn.bias_ih_l1 False\n",
      "mut_encoder rnn.bias_hh_l1 False\n",
      "mut_encoder rnn.weight_ih_l1_reverse False\n",
      "mut_encoder rnn.weight_hh_l1_reverse False\n",
      "mut_encoder rnn.bias_ih_l1_reverse False\n",
      "mut_encoder rnn.bias_hh_l1_reverse False\n",
      "mut_encoder Wz.weight False\n",
      "mut_encoder Wz.bias False\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight True\n",
      "seqlevel_featembeder_HEK We.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "seqlevel_featembeder_K562 We.weight True\n",
      "seqlevel_featembeder_K562 We.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "\n",
      "local_featemb_init_attn_HEK queryv True\n",
      "\n",
      "global_featemb_init_attn_HEK queryv True\n",
      "\n",
      "local_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "global_featemb_mut_attn_HEK queryv True\n",
      "\n",
      "local_featemb_init_attn_K562 queryv True\n",
      "\n",
      "global_featemb_init_attn_K562 queryv True\n",
      "\n",
      "local_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "global_featemb_mut_attn_K562 queryv True\n",
      "\n",
      "decoder_HEK We.weight True\n",
      "decoder_HEK We.bias True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_HEK W_mu.weight True\n",
      "decoder_HEK W_mu.bias True\n",
      "\n",
      "decoder_K562 We.weight True\n",
      "decoder_K562 We.bias True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight True\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight True\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias True\n",
      "decoder_K562 W_mu.weight True\n",
      "decoder_K562 W_mu.bias True\n",
      "\n",
      "seqlevel_featembeder_HEK We.weight 1296\n",
      "seqlevel_featembeder_HEK We.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "seqlevel_featembeder_K562 We.weight 1296\n",
      "seqlevel_featembeder_K562 We.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.0.bias 72\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.weight 5184\n",
      "seqlevel_featembeder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "local_featemb_init_attn_HEK queryv 72\n",
      "global_featemb_init_attn_HEK queryv 72\n",
      "local_featemb_mut_attn_HEK queryv 72\n",
      "global_featemb_mut_attn_HEK queryv 72\n",
      "local_featemb_init_attn_K562 queryv 72\n",
      "global_featemb_init_attn_K562 queryv 72\n",
      "local_featemb_mut_attn_K562 queryv 72\n",
      "global_featemb_mut_attn_K562 queryv 72\n",
      "decoder_HEK We.weight 25920\n",
      "decoder_HEK We.bias 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_HEK encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_HEK encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_HEK W_mu.weight 216\n",
      "decoder_HEK W_mu.bias 3\n",
      "decoder_K562 We.weight 25920\n",
      "decoder_K562 We.bias 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.weight 72\n",
      "decoder_K562 encunit_pipeline.0.layernorm_1.bias 72\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.0.bias 144\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.weight 10368\n",
      "decoder_K562 encunit_pipeline.0.MLP.2.bias 72\n",
      "decoder_K562 W_mu.weight 216\n",
      "decoder_K562 W_mu.bias 3\n",
      "number of parameters of model after freezing: 119238.0\n",
      "119238\n",
      "119238\n",
      "\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 4 | epoch: 0 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [02:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32500\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 4 | epoch: 0 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:08<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3632\n",
      "updated # of rows: 3632\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 4 | epoch: 1 | dsettype: train | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:41<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 32600\n",
      "updated # of rows: 32500\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 4 | epoch: 1 | dsettype: validation | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:05<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 3632\n",
      "updated # of rows: 3632\n"
     ]
    }
   ],
   "source": [
    "# mfolder = 'exp_2023-06-02_09-49-21' # base_90k **\n",
    "# model_type = 'base_90k'\n",
    "# trun = 1 \n",
    "\n",
    "# mfolder = 'exp_2023-08-26_20-58-14' # base_390k **\n",
    "# model_type = 'base_390k'\n",
    "# trun = 1\n",
    "\n",
    "\n",
    "trained_basemodel_dir = os.path.join(repo_dir, \n",
    "                                     'trained_models', \n",
    "                                     model_type,\n",
    "                                     mfolder,\n",
    "                                     'train_val')\n",
    "config_map = (mconfig, options)\n",
    "trmodels_dir_lst = []\n",
    "for base_model_run in [trun]: \n",
    "    time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    tr_val_dir = create_directory(f'exp_{time_stamp}', exp_dir)\n",
    "    state_dict_dir = os.path.join(trained_basemodel_dir, \n",
    "                                  f'run_{base_model_run}',\n",
    "                                 'model_statedict')\n",
    "    trmodels_dir_lst.append(tr_val_dir)\n",
    "    print('basemodel_run:', base_model_run)\n",
    "    print('state_dict_dir:', state_dict_dir)\n",
    "    print('tr_val_dir:', tr_val_dir)\n",
    "    print()\n",
    "    num_epochs=2\n",
    "    tune_trainval_run(dtensor_partitions_multidata,\n",
    "                      config_map, \n",
    "                      tr_val_dir, \n",
    "                      state_dict_dir, \n",
    "                      run_gpu_map, \n",
    "                      num_epochs=num_epochs) # change num_epochs if you want to do a `dry test` (i.e. fast check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-freeware",
   "metadata": {},
   "source": [
    "### Run train models on test set of each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "worth-camera",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating modeldir: /mnt/c/Users/nicol/Documents/GitHub/wsl_experimental_pridict2/experimental_pridict2/experiments/pe_rnn_distribution_multidata/exp_2024-07-11_14-45-14\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 0 | epoch: 0 | dsettype: test | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 9096\n",
      "updated # of rows: 9096\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 1 | epoch: 0 | dsettype: test | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:09<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 9075\n",
      "updated # of rows: 9075\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 2 | epoch: 0 | dsettype: test | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:07<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 9067\n",
      "updated # of rows: 9067\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 3 | epoch: 0 | dsettype: test | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:08<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 9079\n",
      "updated # of rows: 9079\n",
      "{'batch_size': 100, 'num_workers': 0, 'loader_mode': 'cycle', 'datasets_name': ['HEK', 'K562']}\n",
      "loader_mode: cycle\n",
      "datasets_name: ['HEK', 'K562']\n",
      "loss_type: CEloss\n",
      "datasets_name_lst: ['HEK', 'K562']\n",
      "using separate attention layers!!\n",
      "[[(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK')], [(FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562')]]\n",
      "using MLPDecoderDistribution\n",
      "[(AnnotEmbeder_InitSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wproto): Embedding(4, 8, padding_idx=3)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'init_annot_embed'), (AnnotEmbeder_MutSeq(\n",
      "  (We): Embedding(6, 128, padding_idx=5)\n",
      "  (Wpbs): Embedding(3, 8, padding_idx=2)\n",
      "  (Wrt): Embedding(4, 8, padding_idx=3)\n",
      "), 'mut_annot_embed'), (RNN_Net(\n",
      "  (rnn): GRU(152, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'init_encoder'), (RNN_Net(\n",
      "  (rnn): GRU(144, 128, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "  (Wz): Linear(in_features=256, out_features=72, bias=True)\n",
      "  (nonlinear_func): ReLU()\n",
      "), 'mut_encoder'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_HEK'), (MLPEmbedder(\n",
      "  (We): Linear(in_features=18, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=72, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "), 'seqlevel_featembeder_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_HEK'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_init_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'local_featemb_mut_attn_K562'), (FeatureEmbAttention(\n",
      "  (softmax): Softmax(dim=1)\n",
      "), 'global_featemb_mut_attn_K562'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_HEK'), (MLPDecoderDistribution(\n",
      "  (We): Linear(in_features=360, out_features=72, bias=True)\n",
      "  (encunit_pipeline): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (layernorm_1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (MLP): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=144, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=144, out_features=72, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (W_mu): Linear(in_features=72, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      "), 'decoder_K562')]\n",
      "device: cuda:0 | experiment_desc: pe_rnn_distribution_multidata | run_num: 4 | epoch: 0 | dsettype: test | pid: 11698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:08<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__perfmetric_report_multidata_cont__\n",
      "# of rows: 9054\n",
      "updated # of rows: 9054\n"
     ]
    }
   ],
   "source": [
    "config_map = (mconfig, options)\n",
    "for tr_val_dir in trmodels_dir_lst:\n",
    "    print('evaluating modeldir:', tr_val_dir)\n",
    "    test_multidata_run(dtensor_partitions_multidata,\n",
    "                   config_map, \n",
    "                   tr_val_dir, \n",
    "                   tr_val_dir, \n",
    "                   run_gpu_map, \n",
    "                   num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-capitol",
   "metadata": {},
   "source": [
    "### Models' evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "manual-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageedited</th>\n",
       "      <td>0.898341</td>\n",
       "      <td>0.900508</td>\n",
       "      <td>0.896103</td>\n",
       "      <td>0.897903</td>\n",
       "      <td>0.899286</td>\n",
       "      <td>0.898428</td>\n",
       "      <td>0.898341</td>\n",
       "      <td>0.001639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageedited</th>\n",
       "      <td>0.890738</td>\n",
       "      <td>0.892372</td>\n",
       "      <td>0.891015</td>\n",
       "      <td>0.890300</td>\n",
       "      <td>0.891213</td>\n",
       "      <td>0.891128</td>\n",
       "      <td>0.891015</td>\n",
       "      <td>0.000776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageedited  0.898341  0.900508  0.896103  0.897903   \n",
       "pearson_corr_HEK_averageedited   0.890738  0.892372  0.891015  0.890300   \n",
       "\n",
       "                                    run_4      mean    median    stddev  \n",
       "spearman_corr_HEK_averageedited  0.899286  0.898428  0.898341  0.001639  \n",
       "pearson_corr_HEK_averageedited   0.891213  0.891128  0.891015  0.000776  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageedited</th>\n",
       "      <td>0.756520</td>\n",
       "      <td>0.752396</td>\n",
       "      <td>0.752962</td>\n",
       "      <td>0.750411</td>\n",
       "      <td>0.755048</td>\n",
       "      <td>0.753467</td>\n",
       "      <td>0.752962</td>\n",
       "      <td>0.002375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageedited</th>\n",
       "      <td>0.618464</td>\n",
       "      <td>0.608165</td>\n",
       "      <td>0.617701</td>\n",
       "      <td>0.611258</td>\n",
       "      <td>0.601728</td>\n",
       "      <td>0.611463</td>\n",
       "      <td>0.611258</td>\n",
       "      <td>0.006957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageedited  0.756520  0.752396  0.752962  0.750411   \n",
       "pearson_corr_K562_averageedited   0.618464  0.608165  0.617701  0.611258   \n",
       "\n",
       "                                     run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageedited  0.755048  0.753467  0.752962  0.002375  \n",
       "pearson_corr_K562_averageedited   0.601728  0.611463  0.611258  0.006957  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageunedited</th>\n",
       "      <td>0.880484</td>\n",
       "      <td>0.881336</td>\n",
       "      <td>0.878566</td>\n",
       "      <td>0.880294</td>\n",
       "      <td>0.880787</td>\n",
       "      <td>0.880294</td>\n",
       "      <td>0.880484</td>\n",
       "      <td>0.001043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageunedited</th>\n",
       "      <td>0.885028</td>\n",
       "      <td>0.886863</td>\n",
       "      <td>0.885280</td>\n",
       "      <td>0.885090</td>\n",
       "      <td>0.886392</td>\n",
       "      <td>0.885731</td>\n",
       "      <td>0.885280</td>\n",
       "      <td>0.000841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageunedited  0.880484  0.881336  0.878566  0.880294   \n",
       "pearson_corr_HEK_averageunedited   0.885028  0.886863  0.885280  0.885090   \n",
       "\n",
       "                                      run_4      mean    median    stddev  \n",
       "spearman_corr_HEK_averageunedited  0.880787  0.880294  0.880484  0.001043  \n",
       "pearson_corr_HEK_averageunedited   0.886392  0.885731  0.885280  0.000841  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageunedited</th>\n",
       "      <td>0.668958</td>\n",
       "      <td>0.667247</td>\n",
       "      <td>0.664571</td>\n",
       "      <td>0.669594</td>\n",
       "      <td>0.665234</td>\n",
       "      <td>0.667121</td>\n",
       "      <td>0.667247</td>\n",
       "      <td>0.002212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageunedited</th>\n",
       "      <td>0.614942</td>\n",
       "      <td>0.604898</td>\n",
       "      <td>0.613290</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>0.601294</td>\n",
       "      <td>0.608809</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>0.005704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageunedited  0.668958  0.667247  0.664571  0.669594   \n",
       "pearson_corr_K562_averageunedited   0.614942  0.604898  0.613290  0.609621   \n",
       "\n",
       "                                       run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageunedited  0.665234  0.667121  0.667247  0.002212  \n",
       "pearson_corr_K562_averageunedited   0.601294  0.608809  0.609621  0.005704  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageindel</th>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.275885</td>\n",
       "      <td>0.277205</td>\n",
       "      <td>0.273657</td>\n",
       "      <td>0.272809</td>\n",
       "      <td>0.275051</td>\n",
       "      <td>0.27570</td>\n",
       "      <td>0.001784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageindel</th>\n",
       "      <td>0.255856</td>\n",
       "      <td>0.266140</td>\n",
       "      <td>0.278226</td>\n",
       "      <td>0.269608</td>\n",
       "      <td>0.248883</td>\n",
       "      <td>0.263742</td>\n",
       "      <td>0.26614</td>\n",
       "      <td>0.011543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageindel  0.275700  0.275885  0.277205  0.273657   \n",
       "pearson_corr_HEK_averageindel   0.255856  0.266140  0.278226  0.269608   \n",
       "\n",
       "                                   run_4      mean   median    stddev  \n",
       "spearman_corr_HEK_averageindel  0.272809  0.275051  0.27570  0.001784  \n",
       "pearson_corr_HEK_averageindel   0.248883  0.263742  0.26614  0.011543  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageindel</th>\n",
       "      <td>0.231857</td>\n",
       "      <td>0.234924</td>\n",
       "      <td>0.228951</td>\n",
       "      <td>0.232507</td>\n",
       "      <td>0.230886</td>\n",
       "      <td>0.231825</td>\n",
       "      <td>0.231857</td>\n",
       "      <td>0.002191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageindel</th>\n",
       "      <td>0.347141</td>\n",
       "      <td>0.371359</td>\n",
       "      <td>0.358647</td>\n",
       "      <td>0.370484</td>\n",
       "      <td>0.321159</td>\n",
       "      <td>0.353758</td>\n",
       "      <td>0.358647</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageindel  0.231857  0.234924  0.228951  0.232507   \n",
       "pearson_corr_K562_averageindel   0.347141  0.371359  0.358647  0.370484   \n",
       "\n",
       "                                    run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageindel  0.230886  0.231825  0.231857  0.002191  \n",
       "pearson_corr_K562_averageindel   0.321159  0.353758  0.358647  0.020736  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "--- validation ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageedited</th>\n",
       "      <td>0.900594</td>\n",
       "      <td>0.902125</td>\n",
       "      <td>0.910504</td>\n",
       "      <td>0.910543</td>\n",
       "      <td>0.904847</td>\n",
       "      <td>0.905723</td>\n",
       "      <td>0.904847</td>\n",
       "      <td>0.004640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageedited</th>\n",
       "      <td>0.894112</td>\n",
       "      <td>0.890715</td>\n",
       "      <td>0.901314</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.890633</td>\n",
       "      <td>0.893641</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.004517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageedited  0.900594  0.902125  0.910504  0.910543   \n",
       "pearson_corr_HEK_averageedited   0.894112  0.890715  0.901314  0.891432   \n",
       "\n",
       "                                    run_4      mean    median    stddev  \n",
       "spearman_corr_HEK_averageedited  0.904847  0.905723  0.904847  0.004640  \n",
       "pearson_corr_HEK_averageedited   0.890633  0.893641  0.891432  0.004517  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageedited</th>\n",
       "      <td>0.754306</td>\n",
       "      <td>0.767472</td>\n",
       "      <td>0.780294</td>\n",
       "      <td>0.782539</td>\n",
       "      <td>0.775489</td>\n",
       "      <td>0.772020</td>\n",
       "      <td>0.775489</td>\n",
       "      <td>0.011463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageedited</th>\n",
       "      <td>0.584737</td>\n",
       "      <td>0.646825</td>\n",
       "      <td>0.622521</td>\n",
       "      <td>0.612337</td>\n",
       "      <td>0.682693</td>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.622521</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageedited  0.754306  0.767472  0.780294  0.782539   \n",
       "pearson_corr_K562_averageedited   0.584737  0.646825  0.622521  0.612337   \n",
       "\n",
       "                                     run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageedited  0.775489  0.772020  0.775489  0.011463  \n",
       "pearson_corr_K562_averageedited   0.682693  0.629822  0.622521  0.037000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageunedited</th>\n",
       "      <td>0.874954</td>\n",
       "      <td>0.890119</td>\n",
       "      <td>0.896389</td>\n",
       "      <td>0.890129</td>\n",
       "      <td>0.883937</td>\n",
       "      <td>0.887106</td>\n",
       "      <td>0.890119</td>\n",
       "      <td>0.008095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageunedited</th>\n",
       "      <td>0.885184</td>\n",
       "      <td>0.889394</td>\n",
       "      <td>0.893056</td>\n",
       "      <td>0.884760</td>\n",
       "      <td>0.881846</td>\n",
       "      <td>0.886848</td>\n",
       "      <td>0.885184</td>\n",
       "      <td>0.004393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageunedited  0.874954  0.890119  0.896389  0.890129   \n",
       "pearson_corr_HEK_averageunedited   0.885184  0.889394  0.893056  0.884760   \n",
       "\n",
       "                                      run_4      mean    median    stddev  \n",
       "spearman_corr_HEK_averageunedited  0.883937  0.887106  0.890119  0.008095  \n",
       "pearson_corr_HEK_averageunedited   0.881846  0.886848  0.885184  0.004393  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageunedited</th>\n",
       "      <td>0.658835</td>\n",
       "      <td>0.692245</td>\n",
       "      <td>0.676786</td>\n",
       "      <td>0.682347</td>\n",
       "      <td>0.693842</td>\n",
       "      <td>0.680811</td>\n",
       "      <td>0.682347</td>\n",
       "      <td>0.014160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageunedited</th>\n",
       "      <td>0.578394</td>\n",
       "      <td>0.644880</td>\n",
       "      <td>0.620047</td>\n",
       "      <td>0.612094</td>\n",
       "      <td>0.674007</td>\n",
       "      <td>0.625885</td>\n",
       "      <td>0.620047</td>\n",
       "      <td>0.035904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageunedited  0.658835  0.692245  0.676786  0.682347   \n",
       "pearson_corr_K562_averageunedited   0.578394  0.644880  0.620047  0.612094   \n",
       "\n",
       "                                       run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageunedited  0.693842  0.680811  0.682347  0.014160  \n",
       "pearson_corr_K562_averageunedited   0.674007  0.625885  0.620047  0.035904  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageindel</th>\n",
       "      <td>0.261509</td>\n",
       "      <td>0.304135</td>\n",
       "      <td>0.259856</td>\n",
       "      <td>0.318512</td>\n",
       "      <td>0.304080</td>\n",
       "      <td>0.289618</td>\n",
       "      <td>0.304080</td>\n",
       "      <td>0.027068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageindel</th>\n",
       "      <td>0.238272</td>\n",
       "      <td>0.243568</td>\n",
       "      <td>0.243509</td>\n",
       "      <td>0.253626</td>\n",
       "      <td>0.274238</td>\n",
       "      <td>0.250643</td>\n",
       "      <td>0.243568</td>\n",
       "      <td>0.014315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageindel  0.261509  0.304135  0.259856  0.318512   \n",
       "pearson_corr_HEK_averageindel   0.238272  0.243568  0.243509  0.253626   \n",
       "\n",
       "                                   run_4      mean    median    stddev  \n",
       "spearman_corr_HEK_averageindel  0.304080  0.289618  0.304080  0.027068  \n",
       "pearson_corr_HEK_averageindel   0.274238  0.250643  0.243568  0.014315  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageindel</th>\n",
       "      <td>0.205323</td>\n",
       "      <td>0.252111</td>\n",
       "      <td>0.245431</td>\n",
       "      <td>0.296889</td>\n",
       "      <td>0.23285</td>\n",
       "      <td>0.246521</td>\n",
       "      <td>0.245431</td>\n",
       "      <td>0.033368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageindel</th>\n",
       "      <td>0.425064</td>\n",
       "      <td>0.274382</td>\n",
       "      <td>0.310455</td>\n",
       "      <td>0.378761</td>\n",
       "      <td>0.35646</td>\n",
       "      <td>0.349024</td>\n",
       "      <td>0.356460</td>\n",
       "      <td>0.058696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageindel  0.205323  0.252111  0.245431  0.296889   \n",
       "pearson_corr_K562_averageindel   0.425064  0.274382  0.310455  0.378761   \n",
       "\n",
       "                                   run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageindel  0.23285  0.246521  0.245431  0.033368  \n",
       "pearson_corr_K562_averageindel   0.35646  0.349024  0.356460  0.058696  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "--- test ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageedited</th>\n",
       "      <td>0.905339</td>\n",
       "      <td>0.904053</td>\n",
       "      <td>0.898795</td>\n",
       "      <td>0.905389</td>\n",
       "      <td>0.906960</td>\n",
       "      <td>0.904107</td>\n",
       "      <td>0.905339</td>\n",
       "      <td>0.003143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageedited</th>\n",
       "      <td>0.897007</td>\n",
       "      <td>0.898176</td>\n",
       "      <td>0.884071</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>0.902304</td>\n",
       "      <td>0.896509</td>\n",
       "      <td>0.898176</td>\n",
       "      <td>0.007270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageedited  0.905339  0.904053  0.898795  0.905389   \n",
       "pearson_corr_HEK_averageedited   0.897007  0.898176  0.884071  0.900990   \n",
       "\n",
       "                                    run_4      mean    median    stddev  \n",
       "spearman_corr_HEK_averageedited  0.906960  0.904107  0.905339  0.003143  \n",
       "pearson_corr_HEK_averageedited   0.902304  0.896509  0.898176  0.007270  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageedited</th>\n",
       "      <td>0.780155</td>\n",
       "      <td>0.773526</td>\n",
       "      <td>0.774443</td>\n",
       "      <td>0.788912</td>\n",
       "      <td>0.788191</td>\n",
       "      <td>0.781046</td>\n",
       "      <td>0.780155</td>\n",
       "      <td>0.007312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageedited</th>\n",
       "      <td>0.642186</td>\n",
       "      <td>0.631576</td>\n",
       "      <td>0.631518</td>\n",
       "      <td>0.643688</td>\n",
       "      <td>0.642651</td>\n",
       "      <td>0.638324</td>\n",
       "      <td>0.642186</td>\n",
       "      <td>0.006210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageedited  0.780155  0.773526  0.774443  0.788912   \n",
       "pearson_corr_K562_averageedited   0.642186  0.631576  0.631518  0.643688   \n",
       "\n",
       "                                     run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageedited  0.788191  0.781046  0.780155  0.007312  \n",
       "pearson_corr_K562_averageedited   0.642651  0.638324  0.642186  0.006210  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageunedited</th>\n",
       "      <td>0.888964</td>\n",
       "      <td>0.884740</td>\n",
       "      <td>0.879959</td>\n",
       "      <td>0.883876</td>\n",
       "      <td>0.887197</td>\n",
       "      <td>0.884947</td>\n",
       "      <td>0.884740</td>\n",
       "      <td>0.003438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageunedited</th>\n",
       "      <td>0.892276</td>\n",
       "      <td>0.888572</td>\n",
       "      <td>0.880626</td>\n",
       "      <td>0.893647</td>\n",
       "      <td>0.895311</td>\n",
       "      <td>0.890087</td>\n",
       "      <td>0.892276</td>\n",
       "      <td>0.005843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageunedited  0.888964  0.884740  0.879959  0.883876   \n",
       "pearson_corr_HEK_averageunedited   0.892276  0.888572  0.880626  0.893647   \n",
       "\n",
       "                                      run_4      mean    median    stddev  \n",
       "spearman_corr_HEK_averageunedited  0.887197  0.884947  0.884740  0.003438  \n",
       "pearson_corr_HEK_averageunedited   0.895311  0.890087  0.892276  0.005843  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageunedited</th>\n",
       "      <td>0.696549</td>\n",
       "      <td>0.685921</td>\n",
       "      <td>0.698279</td>\n",
       "      <td>0.683989</td>\n",
       "      <td>0.699329</td>\n",
       "      <td>0.692813</td>\n",
       "      <td>0.696549</td>\n",
       "      <td>0.007274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageunedited</th>\n",
       "      <td>0.644663</td>\n",
       "      <td>0.628958</td>\n",
       "      <td>0.633357</td>\n",
       "      <td>0.637798</td>\n",
       "      <td>0.635306</td>\n",
       "      <td>0.636016</td>\n",
       "      <td>0.635306</td>\n",
       "      <td>0.005817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageunedited  0.696549  0.685921  0.698279  0.683989   \n",
       "pearson_corr_K562_averageunedited   0.644663  0.628958  0.633357  0.637798   \n",
       "\n",
       "                                       run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageunedited  0.699329  0.692813  0.696549  0.007274  \n",
       "pearson_corr_K562_averageunedited   0.635306  0.636016  0.635306  0.005817  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_HEK_averageindel</th>\n",
       "      <td>0.301010</td>\n",
       "      <td>0.292837</td>\n",
       "      <td>0.286965</td>\n",
       "      <td>0.289060</td>\n",
       "      <td>0.291004</td>\n",
       "      <td>0.292175</td>\n",
       "      <td>0.291004</td>\n",
       "      <td>0.005402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_HEK_averageindel</th>\n",
       "      <td>0.273591</td>\n",
       "      <td>0.258513</td>\n",
       "      <td>0.268975</td>\n",
       "      <td>0.253349</td>\n",
       "      <td>0.351037</td>\n",
       "      <td>0.281093</td>\n",
       "      <td>0.268975</td>\n",
       "      <td>0.039921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_HEK_averageindel  0.301010  0.292837  0.286965  0.289060   \n",
       "pearson_corr_HEK_averageindel   0.273591  0.258513  0.268975  0.253349   \n",
       "\n",
       "                                   run_4      mean    median    stddev  \n",
       "spearman_corr_HEK_averageindel  0.291004  0.292175  0.291004  0.005402  \n",
       "pearson_corr_HEK_averageindel   0.351037  0.281093  0.268975  0.039921  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_0</th>\n",
       "      <th>run_1</th>\n",
       "      <th>run_2</th>\n",
       "      <th>run_3</th>\n",
       "      <th>run_4</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman_corr_K562_averageindel</th>\n",
       "      <td>0.283801</td>\n",
       "      <td>0.232526</td>\n",
       "      <td>0.275295</td>\n",
       "      <td>0.239439</td>\n",
       "      <td>0.255980</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>0.255980</td>\n",
       "      <td>0.022139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr_K562_averageindel</th>\n",
       "      <td>0.397798</td>\n",
       "      <td>0.320636</td>\n",
       "      <td>0.406359</td>\n",
       "      <td>0.326456</td>\n",
       "      <td>0.437412</td>\n",
       "      <td>0.377732</td>\n",
       "      <td>0.397798</td>\n",
       "      <td>0.051656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    run_0     run_1     run_2     run_3  \\\n",
       "spearman_corr_K562_averageindel  0.283801  0.232526  0.275295  0.239439   \n",
       "pearson_corr_K562_averageindel   0.397798  0.320636  0.406359  0.326456   \n",
       "\n",
       "                                    run_4      mean    median    stddev  \n",
       "spearman_corr_K562_averageindel  0.255980  0.257408  0.255980  0.022139  \n",
       "pearson_corr_K562_averageindel   0.437412  0.377732  0.397798  0.051656  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n"
     ]
    }
   ],
   "source": [
    "dsetnames = dsetnames_lst\n",
    "for dsettype in ('train', 'validation', 'test'):\n",
    "    print(f'--- {dsettype} ---')\n",
    "    for outcome_name in ['averageedited', 'averageunedited', 'averageindel']:\n",
    "        out = build_performance_multidata_dfs(tr_val_dir, 5, dsettype, outcome_name, 'continuous', dsetnames)\n",
    "        for i_data, dsetname in enumerate(dsetnames):\n",
    "            display(out[i_data])\n",
    "            \n",
    "    print('*'*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "buried-buyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2}\n",
      "{'epoch': 2}\n",
      "{'epoch': 2}\n",
      "{'epoch': 2}\n",
      "{'epoch': 2}\n"
     ]
    }
   ],
   "source": [
    "# update options with wsize and seqlevelfeat_dimension\n",
    "tdir = tr_val_dir\n",
    "for run in range(5):\n",
    "    tlink = os.path.join(tdir, 'train_val', f'run_{run}', 'model_statedict', 'best_epoch.pkl')\n",
    "    print(ReaderWriter.read_data(tlink))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
