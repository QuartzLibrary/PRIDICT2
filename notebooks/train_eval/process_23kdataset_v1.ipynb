{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = os.path.join(os.path.abspath('../../'))\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,repo_dir)\n",
    "import pridict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth = os.path.join(repo_dir, 'dataset')\n",
    "data_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_pth, 'proc_v2', f'data_23k_v1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-machine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pridict.pridictv2.dataset import MinMaxNormalizer\n",
    "from pridict.pridictv2.data_preprocess import *\n",
    "from pridict.pridictv2.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcome_colnames(prefix, suffix=None):\n",
    "    if suffix:\n",
    "        lst = [f'{prefix}{colname}_{suffix}' for colname in ['averageedited', 'averageunedited', 'averageunintended']]\n",
    "    else:\n",
    "        lst = [f'{prefix}{colname}' for colname in ['averageedited', 'averageunedited', 'averageunintended']]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-indonesia",
   "metadata": {},
   "source": [
    "### Process the dataset df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-edition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pe_seq_processor = PESeqProcessor()\n",
    "tdf, proc_seq_init_df, num_init_cols,  proc_seq_mut_df, num_mut_cols = pe_seq_processor.process_init_mut_seqs(df, \n",
    "                                                                                                              'wide_initial_target', \n",
    "                                                                                                              'wide_mutated_target', \n",
    "                                                                                                              align_symbol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_editing_alignment_correctness(tdf, correction_len_colname='Correction_Length_effective')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-today",
   "metadata": {},
   "source": [
    "### Merge the new data frame with original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to df\n",
    "df = pd.merge(left = df,\n",
    "              right = tdf[['seq_id', 'wide_initial_target_align', 'wide_mutated_target_align', \n",
    "                           'Correction_Length_effective']],\n",
    "             how='inner',\n",
    "             left_on=['seq_id'],\n",
    "             right_on=['seq_id'])\n",
    "\n",
    "df['PBSinitlength'] = proc_seq_init_df['end_PBS'] - proc_seq_init_df['start_PBS']\n",
    "df['PBSmutlength'] = proc_seq_mut_df['end_PBS'] - proc_seq_mut_df['start_PBS']\n",
    "print((df['PBSinitlength'] == df['PBSmutlength']).all())\n",
    "df['PBSlength'] = df['PBSinitlength']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-impression",
   "metadata": {},
   "source": [
    "### Visualize sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79462b5e",
   "metadata": {},
   "source": [
    "#### using precomputed aligned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "for correction_type in ['Replacement', 'Insertion', 'Deletion']:\n",
    "    cond = (df['Correction_Type'] == correction_type) & (df['Correction_Length']>7)\n",
    "    seq_id = np.random.choice(df.loc[cond, 'seq_id'])\n",
    "    display(HTML(Viz_PESeqs().viz_align_initmut_seq_precomputed(tdf, seq_id, wsize=20, return_type='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a660cdf",
   "metadata": {},
   "source": [
    "#### using original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-canadian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(HTML(Viz_PESeqs().viz_align_initmut_seq(df, seq_id, wsize=20, return_type='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-option",
   "metadata": {},
   "source": [
    "### Normalize the continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_MFE = False\n",
    "include_addendumfeat = False\n",
    "minmax_normalizer = MinMaxNormalizer(include_MFE=include_MFE,include_addendumfeat=include_addendumfeat)\n",
    "norm_colnames = minmax_normalizer.normalize_cont_cols_max(df, suffix='_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PBSlength', 'PBSlength_norm']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['RToverhanglength', 'RToverhanglength_norm']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Correction_Length', 'Correction_Length_norm']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Correction_Length_effective', 'Correction_Length_effective_norm']].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-threshold",
   "metadata": {},
   "source": [
    "### Create datapartitions, datatensor and dump on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pridict.pridictv2.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clean_check_tests(df, dpartitions, outcome_name, suffix=''):\n",
    "    print('run test to check NaN rows are removed for outcome:', outcome_name)\n",
    "    print('>> True would mean that rows are still there!! <<')\n",
    "    for run in range(len(dpartitions)):\n",
    "        print('run:', run)\n",
    "        for dsettype in ['train', 'validation', 'test']:\n",
    "            indices = dpartitions[run][dsettype]\n",
    "            if suffix:\n",
    "                ocols = get_outcome_colnames(outcome_name, suffix)\n",
    "            else:\n",
    "                ocols = get_outcome_colnames(outcome_name)\n",
    "            print(df.loc[indices, ocols].isna().any())\n",
    "        print()\n",
    "    print('run test for confirming there is no overlap between train, validation and test sets')\n",
    "    print('>> 0 means no overlap <<')\n",
    "    for run in range(5):\n",
    "        print('run:', run)\n",
    "        print(df.loc[dpartitions[run]['test'], 'grp_id'].isin(df.loc[dpartitions[run]['train'], 'grp_id']).sum())\n",
    "        print(df.loc[dpartitions[run]['test'], 'grp_id'].isin(df.loc[dpartitions[run]['validation'], 'grp_id']).sum())\n",
    "        print(df.loc[dpartitions[run]['validation'], 'grp_id'].isin(df.loc[dpartitions[run]['train'], 'grp_id']).sum())\n",
    "        \n",
    "def clean_dpartitions(dpartitions, nan_indices):\n",
    "    dpartitions_upd = {}\n",
    "    for run in range(len(dpartitions)):\n",
    "        print('run_id:', run)\n",
    "        dpartitions_upd[run] = {}\n",
    "        for dsettype in ['train', 'validation', 'test']:\n",
    "            indices = dpartitions[run][dsettype]\n",
    "            print(f'# of {dsettype} indices:', len(indices))\n",
    "            clean_indices = set(indices) - set(nan_indices)\n",
    "            print(f'# of {dsettype} indices after:', len(clean_indices))\n",
    "            dpartitions_upd[run][dsettype] = np.array(list(clean_indices))\n",
    "        print()\n",
    "    return dpartitions_upd\n",
    "\n",
    "def plot_y_distrib_acrossfolds(dpartitions, y, opt='separate_folds'):\n",
    "    #  histtype in {'bar', 'step'}, fill=True, stacked=True\n",
    "    if opt == 'separate_dsettypes':\n",
    "        fig, axs = plt.subplots(figsize=(9,11), \n",
    "                                nrows=3, \n",
    "                                constrained_layout=True)\n",
    "        axs = axs.ravel()\n",
    "        for run_num in range(len(dpartitions)):\n",
    "            counter = 0\n",
    "            for dsettype in ['train', 'validation', 'test']:\n",
    "                curr_ax = axs[counter]\n",
    "                ids = dpartitions[run_num][dsettype]\n",
    "                curr_ax.hist(y[ids], alpha=0.3, label=f\"{dsettype}_run{run_num}\")\n",
    "                counter+=1\n",
    "                curr_ax.legend()\n",
    "    elif opt == 'separate_folds':\n",
    "        fig, axs = plt.subplots(figsize=(9,11),\n",
    "                                nrows=5,\n",
    "                                constrained_layout=True)\n",
    "        axs = axs.ravel()\n",
    "        for run_num in range(len(dpartitions)):\n",
    "            curr_ax = axs[run_num]\n",
    "            for dsettype in ['train', 'validation', 'test']:\n",
    "                ids = dpartitions[run_num][dsettype]\n",
    "                curr_ax.hist(y[ids], alpha=0.4,label=f\"{dsettype}_run{run_num}\")\n",
    "                curr_ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-porcelain",
   "metadata": {},
   "source": [
    "### Run to create cleaned datapartitions and dtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-wells",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfolder = 'proc_v2'\n",
    "tdir = create_directory(os.path.join(repo_dir, 'dataset', tfolder))\n",
    "if include_MFE:\n",
    "    fsuffix = 'withMFE'\n",
    "else:\n",
    "    fsuffix = 'withoutMFE'\n",
    "dump_dir = create_directory(os.path.join(repo_dir, 'dataset', tfolder, f'align_{fsuffix}'))\n",
    "hek_indices_nan = ReaderWriter.read_data(os.path.join(tdir, f'hek_indices_nan.pkl'))\n",
    "k562_indices_nan = ReaderWriter.read_data(os.path.join(tdir, f'k562_indices_nan.pkl'))\n",
    "\n",
    "wsize=20\n",
    "outcome_suffix = 'clamped'\n",
    "\n",
    "# get grouped 5-fold data partitions\n",
    "dpartitions = get_stratified_partitions(df['grp_id'].values, num_folds=5, valid_set_portion=0.1, random_state=42)\n",
    "validate_partitions(dpartitions, range(df['grp_id'].shape[0]), valid_set_portion=0.1, test_set_portion=0.2)\n",
    "print()\n",
    "for outcome_name in ['HEK', 'K562']:\n",
    "    dtensor = create_datatensor(df, proc_seq_init_df, num_init_cols, \n",
    "                                proc_seq_mut_df, num_mut_cols, norm_colnames, \n",
    "                                window=wsize, y_ref=get_outcome_colnames(outcome_name, outcome_suffix))\n",
    "    if outcome_name == 'HEK':\n",
    "        nan_indices = hek_indices_nan\n",
    "    elif outcome_name == 'K562':\n",
    "        nan_indices = k562_indices_nan\n",
    "    \n",
    "#     run_clean_check_tests(df, dpartitions, outcome_name, suffix=outcome_suffix)\n",
    "    print()\n",
    "    dpartitions_upd = clean_dpartitions(dpartitions, nan_indices)\n",
    "    run_clean_check_tests(df, dpartitions_upd, outcome_name, suffix=outcome_suffix)\n",
    "    print()\n",
    "    plot_y_distrib_acrossfolds(dpartitions_upd, dtensor.y_score.numpy(), opt='separate_folds')\n",
    "    \n",
    "    # dump on disk\n",
    "    fname = f'dpartitions_{outcome_name}_{outcome_suffix}_wsize{wsize}.pkl'\n",
    "    ReaderWriter.dump_data(dpartitions_upd, os.path.join(dump_dir, fname))\n",
    "    fname = f'dtensor_{outcome_name}_{outcome_suffix}_wsize{wsize}.pkl'\n",
    "    ReaderWriter.dump_data(dtensor, os.path.join(dump_dir, fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526339e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
