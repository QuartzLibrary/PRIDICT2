{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = os.path.join(os.path.abspath('../'))\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,repo_dir)\n",
    "import pridict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth = os.path.join(repo_dir, 'dataset')\n",
    "data_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pth = os.path.join(repo_dir, 'input')\n",
    "input_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pridict.pridictv2.utilities import *\n",
    "from pridict.pridictv2.dataset import *\n",
    "from pridict.pridictv2.predict_outcomedistrib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prieml_model_template():\n",
    "    device = get_device(True, 0)\n",
    "    wsize = 20\n",
    "    normalize_opt = 'max'\n",
    "    # create a model template that will be used to load/instantiate a target trained model\n",
    "    prieml_model = PRIEML_Model(device, wsize=wsize, normalize=normalize_opt, fdtype=torch.float32)\n",
    "    return prieml_model\n",
    "\n",
    "def get_cell_types(model_type):\n",
    "    if model_type == 'base_90k':\n",
    "        return ['HEK']\n",
    "    elif model_type == 'base_390k':\n",
    "        return ['HEKschwank','HEKhyongbum']\n",
    "    \n",
    "def compute_average_predictions(df, grp_cols=['seq_id', 'dataset_name']):\n",
    "    tcols = ['pred_averageedited', 'pred_averageunedited', 'pred_averageindel']\n",
    "    agg_df = df.groupby(by=grp_cols)[tcols].mean()\n",
    "    agg_df.reset_index(inplace=True)\n",
    "    for colname in ('run_num', 'Unnamed: 0', 'model'):\n",
    "        if colname in agg_df:\n",
    "            del agg_df[colname]\n",
    "    return agg_df\n",
    "\n",
    "def deeppridict(pegdataframe, models_lst_dict, model_type='base_90k'):\n",
    "    \"\"\"Perform score prediction on dataframe of features based on RNN model.\n",
    "    \n",
    "    Args:\n",
    "        pegdataframe: pandas DataFrame containing the processed sequence features\n",
    "        models_lst: list of tuples of (pridict_model, model_run_dir)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # setup the dataframe\n",
    "    deepdfcols = ['wide_initial_target', 'wide_mutated_target', 'deepeditposition', \n",
    "                  'deepeditposition_lst', 'Correction_Type', 'Correction_Length', \n",
    "                  'protospacerlocation_only_initial', 'PBSlocation',\n",
    "                  'RT_initial_location', 'RT_mutated_location',\n",
    "                  'RToverhangmatches', 'RToverhanglength', \n",
    "                  'RTlength', 'PBSlength', 'RTmt', 'RToverhangmt','PBSmt','protospacermt',\n",
    "                  'extensionmt','original_base_mt','edited_base_mt','original_base_mt_nan',\n",
    "                  'edited_base_mt_nan']\n",
    "\n",
    "    deepdf = pegdataframe[deepdfcols].copy()\n",
    "    # deepdf.insert(1, 'seq_id', list(range(len(deepdf))))\n",
    "    \n",
    "    for colname in ['protospacerlocation_only_initial', \n",
    "                    'PBSlocation',\n",
    "                    'RT_initial_location',\n",
    "                    'RT_mutated_location',\n",
    "                    'deepeditposition_lst'\n",
    "                   ]:\n",
    "        deepdf[colname] = deepdf[colname].astype('str')\n",
    "\n",
    "    # set mt for deletions to 0:\n",
    "    deepdf['edited_base_mt'] = deepdf.apply(lambda x: 0 if x.Correction_Type == 'Deletion' else x.edited_base_mt,\n",
    "                                            axis=1)\n",
    "    deepdf['original_base_mt'] = deepdf.apply(lambda x: 0 if x.Correction_Type == 'Insertion' else x.original_base_mt,\n",
    "                                              axis=1)\n",
    "    \n",
    "    plain_tcols = ['averageedited', 'averageunedited', 'averageindel']\n",
    "    print(model_type)\n",
    "    cell_types = get_cell_types(model_type)\n",
    "    print(cell_types)\n",
    "    batch_size = int(1500/len(cell_types))\n",
    "    print('successful check -1')\n",
    "    prieml_model = get_prieml_model_template()\n",
    "    print('successful check 0')\n",
    "    # print('deepdf[seq_id]:\\n', deepdf['seq_id'])\n",
    "    # data processing for the same data can be done once given that we already specified the cell_types a priori\n",
    "    dloader = prieml_model.prepare_data(deepdf, \n",
    "                                        None, # since we are specifying cell types model_name can be ignored\n",
    "                                        cell_types=cell_types, \n",
    "                                        y_ref=[], \n",
    "                                        batch_size=batch_size)\n",
    "    \n",
    "    print('successful check 1')\n",
    "    all_avg_preds = {} \n",
    "\n",
    "    for model_id, model_runs_lst in models_lst_dict.items():\n",
    "    \n",
    "        pred_dfs = [] # List to store prediction dataframes for each model\n",
    "        \n",
    "        runs_c = 0\n",
    "        print('successful check 2')\n",
    "        for loaded_model_lst, model_dir in model_runs_lst: # Iterate over each model\n",
    "            # Predict using the current model\n",
    "            print(dloader)\n",
    "            # print('loaded_model_lst:',loaded_model_lst)\n",
    "            print('running model from:', model_dir)\n",
    "            pred_df = prieml_model.predict_from_dloader_using_loaded_models(dloader, loaded_model_lst, y_ref=plain_tcols)\n",
    "            print('successful check 3')\n",
    "            pred_df['run_num'] = runs_c # this is irrelevant as we will average at the end\n",
    "            print('successful check 4')\n",
    "            pred_dfs.append(pred_df) # Append the prediction dataframe to the list\n",
    "            print('successful check 5')\n",
    "            runs_c += 1\n",
    "            # print('pred_df:\\n', pred_df)\n",
    "        # compuate average prediction across runs\n",
    "        pred_df_allruns = pd.concat(pred_dfs, axis=0, ignore_index=True)\n",
    "        print('pred_df_allruns.shape:', pred_df_allruns.shape)\n",
    "        avg_preds = prieml_model.compute_avg_predictions(pred_df_allruns)\n",
    "        print('avg_preds.shape:', avg_preds.shape)\n",
    "        print('successful check 6')\n",
    "\n",
    "        avg_preds['model'] = model_id\n",
    "        print('successful check 7')\n",
    "        # store the average prediction dataframe in for a specified model in a dictionary\n",
    "        all_avg_preds[model_id] = avg_preds\n",
    "        print('successful check 8')\n",
    "    # print('all_avg_predicitons:\\n', all_avg_preds)\n",
    "    return all_avg_preds\n",
    "\n",
    "\n",
    "def load_pridict_model(run_ids=[0], model_type='base_90k'):\n",
    "    \"\"\"construct and return PRIDICT model along with model files directory \"\"\"\n",
    "    models_lst_dict = {}  # Initialize a dictionary to hold lists of models keyed by model_id\n",
    "    repo_dir = os.path.join(os.path.abspath('../'))\n",
    "    if model_type == 'base_90k':\n",
    "        modellist = [\n",
    "        ('base_90k', 'pe_rnn_kldiv', 'exp_2023-06-02_09-49-21')\n",
    "        ]\n",
    "    elif model_type == 'base_390k':\n",
    "        modellist = [\n",
    "            ('base_390k', 'pe_rnn_distribution_multidata', 'exp_2023-08-26_20-58-14')\n",
    "        ]\n",
    "    \n",
    "    # create a model template that will be used to load/instantiate a target trained model\n",
    "    prieml_model = get_prieml_model_template()\n",
    "\n",
    "    for model_desc_tup in modellist:\n",
    "        models_lst = []  # Initialize models_lst for each model\n",
    "        model_id, __, mfolder = model_desc_tup\n",
    "\n",
    "        # mid = f\"{model_id}_{mfolder}\"\n",
    "        mid = model_id\n",
    "        for run_num in run_ids: # add the different model runs (i.e. based on 5 folds)\n",
    "            print('model_id:',model_id)\n",
    "            print('mfolder:',mfolder)\n",
    "            print('run_num:',run_num)\n",
    "            model_dir = os.path.join(repo_dir, 'trained_models', model_id.lower(), mfolder, 'train_val', f'run_{run_num}')\n",
    "            print(model_dir)\n",
    "            cell_types = get_cell_types(model_type)\n",
    "            loaded_model = prieml_model.build_retrieve_models(model_dir, cell_types)\n",
    "            models_lst.append((loaded_model, model_dir))\n",
    "        \n",
    "        models_lst_dict[mid] = models_lst  # Add to the dictionary\n",
    "\n",
    "    return models_lst_dict\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-relationship",
   "metadata": {},
   "source": [
    "### Running on 29k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.read_csv(os.path.join(input_pth, '20240113_librarydiv_df_batchfile_with_adapted_wide_initial_target_with_HEKaverageedited.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['AdVaverageedited'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['HEKaverageedited'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-ranch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(pred_df, dset_names, run_num, model_id, correction_type='all', wsize=20, outcome_names=['averageedited', 'averageunedited', 'averageindel']):\n",
    "    res_lst = []\n",
    "    mscore, report = compute_performance_multidata_from_df(pred_df, dset_names, outcome_names)\n",
    "    for i_data, dsetname in enumerate(dset_names):\n",
    "        m = mscore.modelscores_lst[i_data]\n",
    "        for tindx, tcol in enumerate(outcome_names):\n",
    "            pearson_score = m.pearson_lst[tindx]\n",
    "            spearman_score =  m.spearman_lst[tindx]\n",
    "            res_lst.append([model_id, wsize, run_num, pearson_score, spearman_score, tcol, dsetname, correction_type])\n",
    "    return res_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-acrobat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "coral-cycling",
   "metadata": {},
   "source": [
    "### Evaluate all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-driver",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmodels = ['base_90k','base_390k']\n",
    "pridict2_premadedf = test_df\n",
    "# assign a unique id to each row in the dataframe\n",
    "pridict2_premadedf['seq_id'] = [f'seq_{i}' for i in range(pridict2_premadedf.shape[0])]\n",
    "\n",
    "for model_type in tmodels:\n",
    "    # do 5-fold predictions \n",
    "    run_ids = [0,1,2,3,4]\n",
    "    models_lst_dict = load_pridict_model(run_ids = run_ids, model_type = model_type)\n",
    "    all_avg_preds = deeppridict(pridict2_premadedf, models_lst_dict, model_type)\n",
    "    print('all_avg_preds:\\n', all_avg_preds)\n",
    "\n",
    "    cell_types = get_cell_types(model_type)\n",
    "\n",
    "    #########\n",
    "    # this block is used to take average prediction from ensemble of models \n",
    "    # in case we have separate models, each with multiple trainning runs then we do not need it\n",
    "    # we simply collect the average performance prediction using `agg_df = all_avg_preds[model_type]`\n",
    "    #########\n",
    "\n",
    "    ### Extracting cell types from model\n",
    "    # tmp = [all_avg_preds[model_id] for model_id in all_avg_preds]\n",
    "    # # seq_id, dataset_name, model, predictions cols\n",
    "    # tmp_df = pd.concat(tmp, axis=0, ignore_index=True)\n",
    "    # agg_df = compute_average_predictions(tmp_df, grp_cols=['seq_id', 'dataset_name'])\n",
    "\n",
    "\n",
    "    agg_df = all_avg_preds[model_type]\n",
    "    print('agg_df final shape:', agg_df.shape)\n",
    "    print(agg_df['dataset_name'])\n",
    "    print('pridict2_premadedf.shape:', pridict2_premadedf.shape)\n",
    "    # print('agg_df:\\n', agg_df)\n",
    "    for cell_type in cell_types:\n",
    "        cond  = agg_df['dataset_name'] == cell_type\n",
    "        tmp_df = agg_df.loc[cond, ['seq_id', 'pred_averageedited']].copy()\n",
    "        newcolname = f'{model_type}_editing_Score_deep_{cell_type}'\n",
    "        tmp_df[newcolname] = tmp_df['pred_averageedited']*100\n",
    "        pridict2_premadedf = pd.merge(pridict2_premadedf,\n",
    "                                       tmp_df[['seq_id', newcolname]],\n",
    "                                       how='inner',\n",
    "                                       left_on=['seq_id'],\n",
    "                                       right_on=['seq_id'])\n",
    "        \n",
    "#         pridict2_premadedf.insert(len(pridict2_premadedf.columns), f'{model_type}_editing_Score_deep_{cell_type}', avg_edited_eff)\n",
    "#     pridict2_premadedf.to_csv(f'predictions/20240117_libdiverse_real_long_wide_target_with_{model_type}_model_predictions_5foldaverage.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pridict2_premadedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(df, model_id, dataset_names, ref_outcomename):\n",
    "    res_lst = []\n",
    "    tcol = 'averageedited'\n",
    "\n",
    "    for dset_name in dataset_names:\n",
    "        colname = f\"{model_id}_editing_Score_deep_{dset_name}\"\n",
    "\n",
    "        select_cols = [colname] + [ref_outcomename]\n",
    "        tdf = df[select_cols].copy()\n",
    "        tdf = tdf.dropna(axis=0, subset= [ref_outcomename]) # remove NaN rows for the target reference outcome\n",
    "        \n",
    "        pred_score_arr = tdf[colname]\n",
    "        ref_score_arr = tdf[ref_outcomename]\n",
    "        spearman_corr, pvalue_spc = compute_spearman_corr(pred_score_arr, ref_score_arr)\n",
    "        pearson_corr, pvalue_prc = compute_pearson_corr(pred_score_arr, ref_score_arr)\n",
    "        res_lst.append([model_id, 'avg_run', pearson_corr, spearman_corr, tcol, dset_name, 'all'])\n",
    "    return res_lst\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-patent",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lst = []\n",
    "ref_outcomename = 'HEKaverageedited'\n",
    "for model_type in ['base_90k','base_390k']:\n",
    "    ctypes = get_cell_types(model_type)\n",
    "    print(model_type)\n",
    "    print(ctypes)\n",
    "    res = compute_performance(pridict2_premadedf, model_type, ctypes, ref_outcomename)\n",
    "    res_lst.extend(res)\n",
    "\n",
    "res_df = pd.DataFrame(res_lst)\n",
    "res_df.columns = ['model_id', 'run_num', 'pear_score', 'spearman_score', 'outcome_name', 'cell_type', 'correction_type']\n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-persian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-earth",
   "metadata": {},
   "source": [
    "### Visualizing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['seq_id'] = [f\"seq_{i}\" for i in range(test_df.shape[0])]\n",
    "test_df['deepeditposition_lst'] = test_df['deepeditposition_lst'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-raising",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose ids of sequences to visualize\n",
    "# we are using seq_id as the main column to filter from \n",
    "prieml_model = get_prieml_model_template()\n",
    "viz_res = prieml_model.visualize_seqs(test_df, ['seq_10','seq_50'])\n",
    "for kelm in viz_res:\n",
    "    display(HTML(viz_res[kelm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-qatar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
